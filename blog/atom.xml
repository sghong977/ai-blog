<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://github.com/ai-blog/blog</id>
    <title>SeulGi Hong Blog</title>
    <updated>2023-06-29T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://github.com/ai-blog/blog"/>
    <subtitle>SeulGi Hong Blog</subtitle>
    <icon>https://github.com/ai-blog/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[(REVIEW) Vid2Avatar, CVPR 2023]]></title>
        <id>https://github.com/ai-blog/blog/vid2avatar</id>
        <link href="https://github.com/ai-blog/blog/vid2avatar"/>
        <updated>2023-06-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, I'm going to review the paper for this CVPR 2023, Vid2Avatar.]]></summary>
        <content type="html"><![CDATA[<p>Today, I'm going to review the paper for this CVPR 2023, Vid2Avatar.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vid2avatar-3d-avatar-reconstruction-from-videos-in-the-wild-via-self-supervised-scene-decomposition">"Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition"<a href="#vid2avatar-3d-avatar-reconstruction-from-videos-in-the-wild-via-self-supervised-scene-decomposition" class="hash-link" aria-label="Direct link to &quot;Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition&quot;" title="Direct link to &quot;Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition&quot;">​</a></h4><ul><li>ETH Zürich, CVPR 2023, <a href="https://moygcc.github.io/vid2avatar/" target="_blank" rel="noopener noreferrer">Project Page</a>, <a href="https://files.ait.ethz.ch/projects/vid2avatar/main.pdf" target="_blank" rel="noopener noreferrer">Paper</a></li></ul><h1>Summary</h1><p><img loading="lazy" src="http://files.ait.ethz.ch/projects/vid2avatar/assets/pipeline.png" alt="image" class="img_ev3q"></p><ul><li>casting the segmentation and surface as a joint 3d optimization problem<ul><li>external segmentation model X</li></ul></li><li>spherical space -&gt; sample points are divided into in and out of the sphere</li><li>inner space points -&gt; canonical SDF -&gt; resampling the points by making use of surface information -&gt; canonical texture -&gt; render RGB</li><li>outer space points (randomly sampled) -&gt; SDF (static scene, background) -&gt; combined with dynamic foreground(human)</li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="paper review" term="paper review"/>
        <category label="AI" term="AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Welcome to my blog!]]></title>
        <id>https://github.com/ai-blog/blog/welcome</id>
        <link href="https://github.com/ai-blog/blog/welcome"/>
        <updated>2023-06-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hi, welcome to my blog!]]></summary>
        <content type="html"><![CDATA[<p>Hi, welcome to my blog!</p><img loading="lazy" src="/ai-blog/assets/images/docusaurus-2fd4716b87f8ec012eb561cf8fa3600f.png" width="200" class="img_ev3q"><p>I'll backup my posts in Tistory blog, my old gitblog, git issues (for survey), and other materials.
Everything would be placed here!</p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="main" term="main"/>
        <category label="AI" term="AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[(MY NOTE) How to remove caption from images? - Language SAM]]></title>
        <id>https://github.com/ai-blog/blog/captionSAM</id>
        <link href="https://github.com/ai-blog/blog/captionSAM"/>
        <updated>2023-06-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How to remove caption from the video data?]]></summary>
        <content type="html"><![CDATA[<p>How to remove caption from the video data?</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="task">Task<a href="#task" class="hash-link" aria-label="Direct link to Task" title="Direct link to Task">​</a></h2><ul><li><strong>Goal</strong><ul><li>I want to remove caption area from images</li><li>Output sholud be like bbox, or non-rectangular mask only for the text area. Format doesn't matter but the later one is better to keep more information on remain scene</li></ul></li><li><strong>Issue</strong><ul><li>prefer to use <strong>generalized pretrained model</strong></li><li>Fully Automatic: without any supervision from human (without GUI prompt)</li><li>speed matters</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how">How?<a href="#how" class="hash-link" aria-label="Direct link to How?" title="Direct link to How?">​</a></h2><p><strong>1. OCR</strong>
First of all, optical character recognition would be a nice and trustful approach because it has been a long-lasting computer vision task.</p><ul><li><strong>Scene Text Detection (Localization)</strong>: to get bbox area of the text in the image<ul><li>e.g., CRAFT <a href="https://github.com/clovaai/CRAFT-pytorch" target="_blank" rel="noopener noreferrer">https://github.com/clovaai/CRAFT-pytorch</a> provides the pre-trained model</li></ul></li><li>Scene Text Recognition: I don't need this part</li></ul><p><strong>2. Generalized Segmentation Model</strong>
These days, "Segment Anything" is a trending topic in the field of computer vision. Therefore, I am examining the performance of SAM in masking text areas, and conducting a survey on follow-up studies.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-what-is-sam">Step 1. What is SAM?<a href="#step-1-what-is-sam" class="hash-link" aria-label="Direct link to Step 1. What is SAM?" title="Direct link to Step 1. What is SAM?">​</a></h2><p>I believe the scene text detection task is well-known and has already achieved satisfactory performance. Therefore, I will explore how SAM works for my specific task.</p><ul><li><strong>Generalization</strong>: SAM is general enough to cover a broad set of use cases and can be used out of the box on new image domains — whether underwater photos or cell microscopy — without requiring additional training (a capability often referred to as zero-shot transfer).</li><li><strong>Time Cost</strong>: using light-weight for the real-time inference</li><li><strong>Promptable Segmentation Task</strong>: take various prompts to train for enabling a valid segmentation even with the ambiguous inputs<ul><li>e.g., Forground, Backgroud Points / Bounding Box / Masks / Free-form Text are defined as prompts.</li><li>But free-form text is not released (2023.04.06)</li></ul></li></ul><p><strong>How the model looks like:</strong></p><ul><li>To summarize, the model composed of 3 parts. image encoder, prompt encoder, and decoder.</li><li>image encoder: MAE (Masked Auto-Encoder), takes 1024x1024 sized input, using pretrained ViT</li><li>prompt encoder: the model convert any types of prompt to 256 dim.<ul><li>text: using CLIP embedding</li><li>fg/bg or bbox: using 2 points (NN)</li><li>Mask: using CNN to reduce size and the last, 1*1 conv generates 256 dim code
<img loading="lazy" src="https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/338558258_1349701259095991_4358060436604292355_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=6YFtfSFN9TkAX9KWrZS&amp;_nc_ht=scontent-gmp1-1.xx&amp;oh=00_AfBir_KH9omTbT4lVU6Gx7XOIQ21vb_ytXcNu9ikPmO8XA&amp;oe=649F0109" alt="img" class="img_ev3q"></li></ul></li></ul><p><strong>Dataset: SA-1B</strong></p><ul><li>COCO run-length encoding (RLE) annotation format</li><li>provides 11M images and the huge amount of class agnostic, only for the research purpose (the mask doesn't contains the category!)</li><li>masks are autogenerated by SAM</li><li>you can request validation set, which is randomly sampled and annotated from human annotators.</li></ul><p><strong>My comments:</strong>
I need a fully automatic process but finding fg&amp;bg or bbox would be an additional task. Luckly, I find a nice open-source project to tackle this!</p><p>However, Language SAM is not exactly the same as the original SAM architecture, since Language SAM is relying on text-to-bbox approach by combining SAM and GroundingDINO.</p><ul><li><a href="https://github.com/IDEA-Research/GroundingDINO" target="_blank" rel="noopener noreferrer">GroundingDINO</a></li><li><a href="https://github.com/facebookresearch/segment-anything" target="_blank" rel="noopener noreferrer">Segment-Anything</a></li><li>GUI is available by <a href="https://github.com/Lightning-AI/lightning" target="_blank" rel="noopener noreferrer">Lightning AI</a>! (but i didn't check this)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-lets-try-language-sam">Step 2. Let's try Language SAM<a href="#step-2-lets-try-language-sam" class="hash-link" aria-label="Direct link to Step 2. Let's try Language SAM" title="Direct link to Step 2. Let's try Language SAM">​</a></h2><p>Let's go back to our task, removing caption area.
Language segment anything model: <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">github link</a></p><div class="language-Bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install torch torchvision</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Outputs</strong></p><ul><li>The output is consists of instances, and each of them contain <strong>binary mask, bounding box, phrase, logit</strong>.
<img loading="lazy" src="https://github.com/sghong977/sghong977.github.io/assets/46152199/39aacc96-cea3-4ec6-bb6e-b31f7fcf146e" alt="image" class="img_ev3q"></li></ul><p>I didn't upload the output due to the copyright issue, but I'll add some visualization later on.</p><p>References</p><ul><li>Language SAM <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">https://github.com/luca-medeiros/lang-segment-anything</a></li><li><a href="https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/" target="_blank" rel="noopener noreferrer">https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/</a></li><li>facebook <a href="https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/?ref=blog.annotation-ai.com" target="_blank" rel="noopener noreferrer">post</a></li><li>(KR) Explanation about SAM details <a href="https://blog.annotation-ai.com/segment-anything/" target="_blank" rel="noopener noreferrer">https://blog.annotation-ai.com/segment-anything/</a></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Segment Anything Model" term="Segment Anything Model"/>
        <category label="Text Segmentation" term="Text Segmentation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[(DEBUG) Settings]]></title>
        <id>https://github.com/ai-blog/blog/settings</id>
        <link href="https://github.com/ai-blog/blog/settings"/>
        <updated>2023-04-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Stressful Settings...]]></summary>
        <content type="html"><![CDATA[<p>Stressful Settings...</p><h1>Dependencies</h1><ul><li>Detectron2<ul><li>python 3.7 &gt;=</li><li>torch 1.8 &gt;=</li></ul></li><li>scikit-learn == 0.22<ul><li>it takes a while to set</li><li>pip: Cython</li><li>apt-get: gcc, g++<ul><li><a href="https://woowaa.net/entry/gcc-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0" target="_blank" rel="noopener noreferrer">link</a></li></ul></li></ul></li><li>official neural renderer is tensorflow version.</li></ul><hr><h1>Errors</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="importerror-libglso1-cannot-open-shared-object-file-no-such-file-or-directory">"ImportError: libGL.so.1: cannot open shared object file: No such file or directory"<a href="#importerror-libglso1-cannot-open-shared-object-file-no-such-file-or-directory" class="hash-link" aria-label="Direct link to &quot;ImportError: libGL.so.1: cannot open shared object file: No such file or directory&quot;" title="Direct link to &quot;ImportError: libGL.so.1: cannot open shared object file: No such file or directory&quot;">​</a></h3><p><code>apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6  -y</code></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="importerror-unable-to-load-opengl-library-osmesa-cannot-open-shared-object-file-no-such-file-or-directory-osmesa-none">ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)<a href="#importerror-unable-to-load-opengl-library-osmesa-cannot-open-shared-object-file-no-such-file-or-directory-osmesa-none" class="hash-link" aria-label="Direct link to ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)" title="Direct link to ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)">​</a></h3><ul><li>install OSMesa</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="attributeerror-posixpath-object-has-no-attribute-path">AttributeError: 'PosixPath' object has no attribute 'path'<a href="#attributeerror-posixpath-object-has-no-attribute-path" class="hash-link" aria-label="Direct link to AttributeError: 'PosixPath' object has no attribute 'path'" title="Direct link to AttributeError: 'PosixPath' object has no attribute 'path'">​</a></h3><ul><li><a href="https://stackoverflow.com/questions/59693174/attributeerror-posixpath-object-has-no-attribute-path" target="_blank" rel="noopener noreferrer">link</a></li><li><a href="https://ryanking13.github.io/2018/05/22/pathlib.html" target="_blank" rel="noopener noreferrer">link2</a></li><li>PosixPath? When I print os.name, 'posix' returned.</li></ul><hr><h1>Docker</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-delete-none-docker-images">How to delete <!-- -->&lt;<!-- -->none<!-- -->&gt;<!-- --> docker images?<a href="#how-to-delete-none-docker-images" class="hash-link" aria-label="Direct link to how-to-delete-none-docker-images" title="Direct link to how-to-delete-none-docker-images">​</a></h3><p><code>docker rmi $(docker images -f "dangling=true" -q)</code></p><p>After I tried this, every <!-- -->&lt;<!-- -->none<!-- -->&gt;<!-- --> dockers are deleted.</p><hr><h1>Installation</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="openpose">OpenPose<a href="#openpose" class="hash-link" aria-label="Direct link to OpenPose" title="Direct link to OpenPose">​</a></h3><ul><li>Install guide <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source" target="_blank" rel="noopener noreferrer">link</a>, <a href="https://jjeamin.github.io/posts/openpose/" target="_blank" rel="noopener noreferrer">link2</a></li><li>I'm using Ubuntu server and don't have GUI</li><li>git clone regressively.</li><li>.... I just download openpose docker image. docker pull d0ckaaa/openpose</li><li>cmake ..</li><li>make -j4<code>nproc</code></li><li>how to use: <a href="https://viso.ai/deep-learning/openpose/" target="_blank" rel="noopener noreferrer">https://viso.ai/deep-learning/openpose/</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="osmesa">OSMesa<a href="#osmesa" class="hash-link" aria-label="Direct link to OSMesa" title="Direct link to OSMesa">​</a></h3><ul><li>Try this. <a href="https://pyrender.readthedocs.io/en/latest/install/index.html#installmesa" target="_blank" rel="noopener noreferrer">link</a></li><li>Before try this,<ul><li>apt-get install wget</li><li>apt-get install pkg-config</li><li>apt-get install libexpat1-dev</li></ul></li><li>i installed in /usr/local</li><li>MESA_HOME=/usr/local (+/lib?)</li><li>issues<ul><li><a href="https://redstarhong.tistory.com/98" target="_blank" rel="noopener noreferrer">https://redstarhong.tistory.com/98</a> GL? PyOpenGL?</li><li>update "/opt/conda/lib/python3.9/site-packages/OpenGL/platform/egl.py"</li><li>apt-get install libosmesa6-dev</li><li>if PyOpenGL is failed, clone git and reinstall PyOpenGL.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="phalp">PHALP<a href="#phalp" class="hash-link" aria-label="Direct link to PHALP" title="Direct link to PHALP">​</a></h3><p>I failed while using conda yml file because:</p><ul><li>First, I installed conda inside of a docker container to keep experiments going on.<ul><li>when I tried to create conda env, it takes more than an hour to solve dependencies, but it still doesn't finished.</li></ul></li><li>Then, I tried to install every commands on my own, neural renderer and cuda path problems occurred.<ul><li>It was hard for me to find cuda path in conda env because someone said that each conda environment would only contain the parts of cuda</li><li>So I should export every paths of cuda related stuff, but when I searched cuda paths in my server computer, there are too many other settings, so I'm not sure what to add</li></ul></li><li>These are the reasons why I didn't use conda</li></ul><p>I use these commands sequentially. After that, demo works well on my docker container.
Official PHALP's colab pip output: <a href="https://github.com/sghong977/sghong977.github.io/files/11175914/phalp_colab.txt" target="_blank" rel="noopener noreferrer">phalp_colab.txt</a>  </p><p>! pip install pyrender<br>
<!-- -->! pip install opencv-python<br>
<!-- -->! pip install joblib<br>
<!-- -->! pip install cython<br>
<!-- -->! pip install scikit-learn==0.22<br>
<!-- -->! pip install scikit-image<br>
<!-- -->! pip install git+<a href="https://github.com/facebookresearch/detectron2.git" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/detectron2.git</a><br>
<!-- -->! pip install git+<a href="https://github.com/brjathu/pytube.git" target="_blank" rel="noopener noreferrer">https://github.com/brjathu/pytube.git</a><br>
<!-- -->! pip install git+<a href="https://github.com/brjathu/NMR.git" target="_blank" rel="noopener noreferrer">https://github.com/brjathu/NMR.git</a><br>
<!-- -->! pip install chumpy<br>
<!-- -->! pip install ipython<br>
<!-- -->! pip install gdown<br>
<!-- -->! pip install dill<br>
<!-- -->! pip install scenedetect<!-- -->[opencv]<!-- -->  </p><p>But if you use v1.1, additional packages are needed.</p><ul><li>Install OSMesa</li><li>pip install smplx submitit rich pyrootutils colordict</li><li>pip install -e .</li><li>install hydra-core again (Posix error)</li><li>python scripts/demo.py video.source=\'"<a href="https://www.youtube.com/watch?v=xEH_5T9jMVU%22%5C'" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=xEH_5T9jMVU"\'</a><ul><li>I use osmesa, not egl
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/230560634-e9edb760-e566-4d5c-88c1-ea38c4cf3855.png" alt="image" class="img_ev3q"></li></ul></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="installation" term="installation"/>
    </entry>
</feed>