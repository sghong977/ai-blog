<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://github.com/ai-blog/blog</id>
    <title>SeulGi Hong Blog</title>
    <updated>2023-06-29T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://github.com/ai-blog/blog"/>
    <subtitle>SeulGi Hong Blog</subtitle>
    <icon>https://github.com/ai-blog/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Vid2Avatar, CVPR 2023]]></title>
        <id>https://github.com/ai-blog/blog/vid2avatar</id>
        <link href="https://github.com/ai-blog/blog/vid2avatar"/>
        <updated>2023-06-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, I'm going to review the paper for this CVPR 2023, Vid2Avatar.]]></summary>
        <content type="html"><![CDATA[<p>Today, I'm going to review the paper for this CVPR 2023, Vid2Avatar.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vid2avatar-3d-avatar-reconstruction-from-videos-in-the-wild-via-self-supervised-scene-decomposition">"Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition"<a href="#vid2avatar-3d-avatar-reconstruction-from-videos-in-the-wild-via-self-supervised-scene-decomposition" class="hash-link" aria-label="Direct link to &quot;Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition&quot;" title="Direct link to &quot;Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition&quot;">​</a></h4><ul><li>ETH Zürich, CVPR 2023, <a href="https://moygcc.github.io/vid2avatar/" target="_blank" rel="noopener noreferrer">Project Page</a>, <a href="https://files.ait.ethz.ch/projects/vid2avatar/main.pdf" target="_blank" rel="noopener noreferrer">Paper</a></li></ul><h1>Summary</h1><p><img loading="lazy" src="http://files.ait.ethz.ch/projects/vid2avatar/assets/pipeline.png" alt="image" class="img_ev3q"></p><ul><li>casting the segmentation and surface as a joint 3d optimization problem<ul><li>external segmentation model X</li></ul></li><li>spherical space -&gt; sample points are divided into in and out of the sphere</li><li>inner space points -&gt; canonical SDF -&gt; resampling the points by making use of surface information -&gt; canonical texture -&gt; render RGB</li><li>outer space points (randomly sampled) -&gt; SDF (static scene, background) -&gt; combined with dynamic foreground(human)</li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Paper Review" term="Paper Review"/>
        <category label="NeRF" term="NeRF"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Welcome to my blog!]]></title>
        <id>https://github.com/ai-blog/blog/welcome</id>
        <link href="https://github.com/ai-blog/blog/welcome"/>
        <updated>2023-06-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hi, welcome to my blog!]]></summary>
        <content type="html"><![CDATA[<p>Hi, welcome to my blog!</p><img loading="lazy" src="/ai-blog/assets/images/docusaurus-2fd4716b87f8ec012eb561cf8fa3600f.png" width="200" class="img_ev3q"><p>I'll backup my posts in...</p><ul class="contains-task-list containsTaskList_mC6p"><li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->Tistory blog</li><li class="task-list-item"><input type="checkbox" checked="" disabled=""> <!-- -->my old gitblog</li><li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->git issues (for survey)</li><li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->and other materials.
Everything would be placed here!</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="categories">Categories<a href="#categories" class="hash-link" aria-label="Direct link to Categories" title="Direct link to Categories">​</a></h2><p>categories are notated in the first tag of each posts.</p><ul><li>Paper Review: in the <code>paper-reviews</code> dir</li><li>Research Note: in the <code>research-note</code> dir</li><li>Debug: in the <code>settings</code> dir. annoying dependencies, installation tips</li><li></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="main" term="main"/>
        <category label="AI" term="AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to remove caption from images? - Language SAM]]></title>
        <id>https://github.com/ai-blog/blog/captionSAM</id>
        <link href="https://github.com/ai-blog/blog/captionSAM"/>
        <updated>2023-06-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How to remove caption from the video data?]]></summary>
        <content type="html"><![CDATA[<p>How to remove caption from the video data?</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="task">Task<a href="#task" class="hash-link" aria-label="Direct link to Task" title="Direct link to Task">​</a></h2><ul><li><strong>Goal</strong><ul><li>I want to remove caption area from images</li><li>Output sholud be like bbox, or non-rectangular mask only for the text area. Format doesn't matter but the later one is better to keep more information on remain scene</li></ul></li><li><strong>Issue</strong><ul><li>prefer to use <strong>generalized pretrained model</strong></li><li>Fully Automatic: without any supervision from human (without GUI prompt)</li><li>speed matters</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how">How?<a href="#how" class="hash-link" aria-label="Direct link to How?" title="Direct link to How?">​</a></h2><p><strong>1. OCR</strong>
First of all, optical character recognition would be a nice and trustful approach because it has been a long-lasting computer vision task.</p><ul><li><strong>Scene Text Detection (Localization)</strong>: to get bbox area of the text in the image<ul><li>e.g., CRAFT <a href="https://github.com/clovaai/CRAFT-pytorch" target="_blank" rel="noopener noreferrer">https://github.com/clovaai/CRAFT-pytorch</a> provides the pre-trained model</li></ul></li><li>Scene Text Recognition: I don't need this part</li></ul><p><strong>2. Generalized Segmentation Model</strong>
These days, "Segment Anything" is a trending topic in the field of computer vision. Therefore, I am examining the performance of SAM in masking text areas, and conducting a survey on follow-up studies.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-what-is-sam">Step 1. What is SAM?<a href="#step-1-what-is-sam" class="hash-link" aria-label="Direct link to Step 1. What is SAM?" title="Direct link to Step 1. What is SAM?">​</a></h2><p>I believe the scene text detection task is well-known and has already achieved satisfactory performance. Therefore, I will explore how SAM works for my specific task.</p><ul><li><strong>Generalization</strong>: SAM is general enough to cover a broad set of use cases and can be used out of the box on new image domains — whether underwater photos or cell microscopy — without requiring additional training (a capability often referred to as zero-shot transfer).</li><li><strong>Time Cost</strong>: using light-weight for the real-time inference</li><li><strong>Promptable Segmentation Task</strong>: take various prompts to train for enabling a valid segmentation even with the ambiguous inputs<ul><li>e.g., Forground, Backgroud Points / Bounding Box / Masks / Free-form Text are defined as prompts.</li><li>But free-form text is not released (2023.04.06)</li></ul></li></ul><p><strong>How the model looks like:</strong></p><ul><li>To summarize, the model composed of 3 parts. image encoder, prompt encoder, and decoder.</li><li>image encoder: MAE (Masked Auto-Encoder), takes 1024x1024 sized input, using pretrained ViT</li><li>prompt encoder: the model convert any types of prompt to 256 dim.<ul><li>text: using CLIP embedding</li><li>fg/bg or bbox: using 2 points (NN)</li><li>Mask: using CNN to reduce size and the last, 1*1 conv generates 256 dim code
<img loading="lazy" src="https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/338558258_1349701259095991_4358060436604292355_n.png?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=ad8a9d&amp;_nc_ohc=6YFtfSFN9TkAX9KWrZS&amp;_nc_ht=scontent-gmp1-1.xx&amp;oh=00_AfBir_KH9omTbT4lVU6Gx7XOIQ21vb_ytXcNu9ikPmO8XA&amp;oe=649F0109" alt="img" class="img_ev3q"></li></ul></li></ul><p><strong>Dataset: SA-1B</strong></p><ul><li>COCO run-length encoding (RLE) annotation format</li><li>provides 11M images and the huge amount of class agnostic, only for the research purpose (the mask doesn't contains the category!)</li><li>masks are autogenerated by SAM</li><li>you can request validation set, which is randomly sampled and annotated from human annotators.</li></ul><p><strong>My comments:</strong>
I need a fully automatic process but finding fg&amp;bg or bbox would be an additional task. Luckly, I find a nice open-source project to tackle this!</p><p>However, Language SAM is not exactly the same as the original SAM architecture, since Language SAM is relying on text-to-bbox approach by combining SAM and GroundingDINO.</p><ul><li><a href="https://github.com/IDEA-Research/GroundingDINO" target="_blank" rel="noopener noreferrer">GroundingDINO</a></li><li><a href="https://github.com/facebookresearch/segment-anything" target="_blank" rel="noopener noreferrer">Segment-Anything</a></li><li>GUI is available by <a href="https://github.com/Lightning-AI/lightning" target="_blank" rel="noopener noreferrer">Lightning AI</a>! (but i didn't check this)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-lets-try-language-sam">Step 2. Let's try Language SAM<a href="#step-2-lets-try-language-sam" class="hash-link" aria-label="Direct link to Step 2. Let's try Language SAM" title="Direct link to Step 2. Let's try Language SAM">​</a></h2><p>Let's go back to our task, removing caption area.
Language segment anything model: <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">github link</a></p><div class="language-Bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip install torch torchvision</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Outputs</strong></p><ul><li>The output is consists of instances, and each of them contain <strong>binary mask, bounding box, phrase, logit</strong>.
<img loading="lazy" src="https://github.com/sghong977/sghong977.github.io/assets/46152199/39aacc96-cea3-4ec6-bb6e-b31f7fcf146e" alt="image" class="img_ev3q"></li></ul><p>I didn't upload the output due to the copyright issue, but I'll add some visualization later on.</p><p>References</p><ul><li>Language SAM <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">https://github.com/luca-medeiros/lang-segment-anything</a></li><li><a href="https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/" target="_blank" rel="noopener noreferrer">https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/</a></li><li>facebook <a href="https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/?ref=blog.annotation-ai.com" target="_blank" rel="noopener noreferrer">post</a></li><li>(KR) Explanation about SAM details <a href="https://blog.annotation-ai.com/segment-anything/" target="_blank" rel="noopener noreferrer">https://blog.annotation-ai.com/segment-anything/</a></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Segment Anything Model" term="Segment Anything Model"/>
        <category label="Text Segmentation" term="Text Segmentation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trends on April 2023]]></title>
        <id>https://github.com/ai-blog/blog/2304</id>
        <link href="https://github.com/ai-blog/blog/2304"/>
        <updated>2023-04-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[AI-related things that I got on this week.]]></summary>
        <content type="html"><![CDATA[<p>AI-related things that I got on this week.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="monoavatar">MonoAvatar<a href="#monoavatar" class="hash-link" aria-label="Direct link to MonoAvatar" title="Direct link to MonoAvatar">​</a></h2><ul><li>Monocular RGB video -&gt; volumetric photorealistic 3D avatar</li><li>can be rendered with user-defined expression and viewpoint.</li><li>CVPR 23, Google and Simon Fraser University <a href="https://augmentedperception.github.io/monoavatar/" target="_blank" rel="noopener noreferrer">link</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="segment-anything">Segment Anything<a href="#segment-anything" class="hash-link" aria-label="Direct link to Segment Anything" title="Direct link to Segment Anything">​</a></h2><ul><li><a href="https://github.com/facebookresearch/segment-anything" target="_blank" rel="noopener noreferrer">Facebook Research</a></li><li><a href="https://blog.annotation-ai.com/segment-anything/" target="_blank" rel="noopener noreferrer">kor explain. link</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3d-object-rigging-from-a-single-image">3D Object Rigging from a Single Image<a href="#3d-object-rigging-from-a-single-image" class="hash-link" aria-label="Direct link to 3D Object Rigging from a Single Image" title="Direct link to 3D Object Rigging from a Single Image">​</a></h2><ul><li>CVPR 19 <a href="https://developer.nvidia.com/blog/transforming-paintings-and-photos-into-animations-with-ai/" target="_blank" rel="noopener noreferrer">animate pic</a></li><li>ECCV 22 <a href="https://kulbear.github.io/object-wakeup/" target="_blank" rel="noopener noreferrer">obj wakeup</a></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Trends" term="Trends"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annoying Settings]]></title>
        <id>https://github.com/ai-blog/blog/settings</id>
        <link href="https://github.com/ai-blog/blog/settings"/>
        <updated>2023-04-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Stressful Settings...]]></summary>
        <content type="html"><![CDATA[<p>Stressful Settings...</p><h1>Dependencies</h1><ul><li>Detectron2<ul><li>python 3.7 &gt;=</li><li>torch 1.8 &gt;=</li></ul></li><li>scikit-learn == 0.22<ul><li>it takes a while to set</li><li>pip: Cython</li><li>apt-get: gcc, g++<ul><li><a href="https://woowaa.net/entry/gcc-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0" target="_blank" rel="noopener noreferrer">link</a></li></ul></li></ul></li><li>official neural renderer is tensorflow version.</li></ul><hr><h1>Errors</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="importerror-libglso1-cannot-open-shared-object-file-no-such-file-or-directory">"ImportError: libGL.so.1: cannot open shared object file: No such file or directory"<a href="#importerror-libglso1-cannot-open-shared-object-file-no-such-file-or-directory" class="hash-link" aria-label="Direct link to &quot;ImportError: libGL.so.1: cannot open shared object file: No such file or directory&quot;" title="Direct link to &quot;ImportError: libGL.so.1: cannot open shared object file: No such file or directory&quot;">​</a></h3><p><code>apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6  -y</code></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="importerror-unable-to-load-opengl-library-osmesa-cannot-open-shared-object-file-no-such-file-or-directory-osmesa-none">ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)<a href="#importerror-unable-to-load-opengl-library-osmesa-cannot-open-shared-object-file-no-such-file-or-directory-osmesa-none" class="hash-link" aria-label="Direct link to ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)" title="Direct link to ImportError: ('Unable to load OpenGL library', 'OSMesa: cannot open shared object file: No such file or directory', 'OSMesa', None)">​</a></h3><ul><li>install OSMesa</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="attributeerror-posixpath-object-has-no-attribute-path">AttributeError: 'PosixPath' object has no attribute 'path'<a href="#attributeerror-posixpath-object-has-no-attribute-path" class="hash-link" aria-label="Direct link to AttributeError: 'PosixPath' object has no attribute 'path'" title="Direct link to AttributeError: 'PosixPath' object has no attribute 'path'">​</a></h3><ul><li><a href="https://stackoverflow.com/questions/59693174/attributeerror-posixpath-object-has-no-attribute-path" target="_blank" rel="noopener noreferrer">link</a></li><li><a href="https://ryanking13.github.io/2018/05/22/pathlib.html" target="_blank" rel="noopener noreferrer">link2</a></li><li>PosixPath? When I print os.name, 'posix' returned.</li></ul><hr><h1>Docker</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-delete-none-docker-images">How to delete <!-- -->&lt;<!-- -->none<!-- -->&gt;<!-- --> docker images?<a href="#how-to-delete-none-docker-images" class="hash-link" aria-label="Direct link to how-to-delete-none-docker-images" title="Direct link to how-to-delete-none-docker-images">​</a></h3><p><code>docker rmi $(docker images -f "dangling=true" -q)</code></p><p>After I tried this, every <!-- -->&lt;<!-- -->none<!-- -->&gt;<!-- --> dockers are deleted.</p><hr><h1>Installation</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="openpose">OpenPose<a href="#openpose" class="hash-link" aria-label="Direct link to OpenPose" title="Direct link to OpenPose">​</a></h3><ul><li>Install guide <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source" target="_blank" rel="noopener noreferrer">link</a>, <a href="https://jjeamin.github.io/posts/openpose/" target="_blank" rel="noopener noreferrer">link2</a></li><li>I'm using Ubuntu server and don't have GUI</li><li>git clone regressively.</li><li>.... I just download openpose docker image. docker pull d0ckaaa/openpose</li><li>cmake ..</li><li>make -j4<code>nproc</code></li><li>how to use: <a href="https://viso.ai/deep-learning/openpose/" target="_blank" rel="noopener noreferrer">https://viso.ai/deep-learning/openpose/</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="osmesa">OSMesa<a href="#osmesa" class="hash-link" aria-label="Direct link to OSMesa" title="Direct link to OSMesa">​</a></h3><ul><li>Try this. <a href="https://pyrender.readthedocs.io/en/latest/install/index.html#installmesa" target="_blank" rel="noopener noreferrer">link</a></li><li>Before try this,<ul><li>apt-get install wget</li><li>apt-get install pkg-config</li><li>apt-get install libexpat1-dev</li></ul></li><li>i installed in /usr/local</li><li>MESA_HOME=/usr/local (+/lib?)</li><li>issues<ul><li><a href="https://redstarhong.tistory.com/98" target="_blank" rel="noopener noreferrer">https://redstarhong.tistory.com/98</a> GL? PyOpenGL?</li><li>update "/opt/conda/lib/python3.9/site-packages/OpenGL/platform/egl.py"</li><li>apt-get install libosmesa6-dev</li><li>if PyOpenGL is failed, clone git and reinstall PyOpenGL.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="phalp">PHALP<a href="#phalp" class="hash-link" aria-label="Direct link to PHALP" title="Direct link to PHALP">​</a></h3><p>I failed while using conda yml file because:</p><ul><li>First, I installed conda inside of a docker container to keep experiments going on.<ul><li>when I tried to create conda env, it takes more than an hour to solve dependencies, but it still doesn't finished.</li></ul></li><li>Then, I tried to install every commands on my own, neural renderer and cuda path problems occurred.<ul><li>It was hard for me to find cuda path in conda env because someone said that each conda environment would only contain the parts of cuda</li><li>So I should export every paths of cuda related stuff, but when I searched cuda paths in my server computer, there are too many other settings, so I'm not sure what to add</li></ul></li><li>These are the reasons why I didn't use conda</li></ul><p>I use these commands sequentially. After that, demo works well on my docker container.
Official PHALP's colab pip output: <a href="https://github.com/sghong977/sghong977.github.io/files/11175914/phalp_colab.txt" target="_blank" rel="noopener noreferrer">phalp_colab.txt</a>  </p><p>! pip install pyrender<br>
<!-- -->! pip install opencv-python<br>
<!-- -->! pip install joblib<br>
<!-- -->! pip install cython<br>
<!-- -->! pip install scikit-learn==0.22<br>
<!-- -->! pip install scikit-image<br>
<!-- -->! pip install git+<a href="https://github.com/facebookresearch/detectron2.git" target="_blank" rel="noopener noreferrer">https://github.com/facebookresearch/detectron2.git</a><br>
<!-- -->! pip install git+<a href="https://github.com/brjathu/pytube.git" target="_blank" rel="noopener noreferrer">https://github.com/brjathu/pytube.git</a><br>
<!-- -->! pip install git+<a href="https://github.com/brjathu/NMR.git" target="_blank" rel="noopener noreferrer">https://github.com/brjathu/NMR.git</a><br>
<!-- -->! pip install chumpy<br>
<!-- -->! pip install ipython<br>
<!-- -->! pip install gdown<br>
<!-- -->! pip install dill<br>
<!-- -->! pip install scenedetect<!-- -->[opencv]<!-- -->  </p><p>But if you use v1.1, additional packages are needed.</p><ul><li>Install OSMesa</li><li>pip install smplx submitit rich pyrootutils colordict</li><li>pip install -e .</li><li>install hydra-core again (Posix error)</li><li>python scripts/demo.py video.source=\'"<a href="https://www.youtube.com/watch?v=xEH_5T9jMVU%22%5C'" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=xEH_5T9jMVU"\'</a><ul><li>I use osmesa, not egl
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/230560634-e9edb760-e566-4d5c-88c1-ea38c4cf3855.png" alt="image" class="img_ev3q"></li></ul></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Debug" term="Debug"/>
        <category label="Installation" term="Installation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multishot Human Pose]]></title>
        <id>https://github.com/ai-blog/blog/mshot</id>
        <link href="https://github.com/ai-blog/blog/mshot"/>
        <updated>2023-04-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[I'm trying to extract SMPL parameters on video in a multishot setting. To this end, tracklets are necessary.]]></summary>
        <content type="html"><![CDATA[<p>I'm trying to extract SMPL parameters on video in a multishot setting. To this end, tracklets are necessary.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="steps">Steps<a href="#steps" class="hash-link" aria-label="Direct link to Steps" title="Direct link to Steps">​</a></h3><p>PHALP setup -&gt; Multishot Human Pose extraction -&gt; select data for training NeRF.</p><h1>1. PHALP setup</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="summary-of-phalp">Summary of PHALP<a href="#summary-of-phalp" class="hash-link" aria-label="Direct link to Summary of PHALP" title="Direct link to Summary of PHALP">​</a></h3><ul><li>"Tracking People by Predicting 3D Appearance, Location &amp; Pose" (CVPR 2022)</li><li>Input: monocular video</li><li>Task: Tracking people</li><li>Method<ul><li>lift people in a frame to 3D (3D pose of human, human location in 3D space, 3D appearance)</li><li>track a person: collect 3D observations over time in a tracklet representation -&gt; build temporal model<ul><li>model predict the <strong>future state of a tracklet including 3d location, pose, appearance</strong></li><li>for the future frame, calculate similarity between pred state of tracklet and single frame observations</li></ul></li><li>Association: simple Hungarian matching</li></ul></li><li>Output: *.pkl file (input for the next step, multishot human pose extraction)</li><li>project page: <a href="http://people.eecs.berkeley.edu/~jathushan/PHALP/" target="_blank" rel="noopener noreferrer">PHALP project</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="installation">Installation<a href="#installation" class="hash-link" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h3><p><strong>Tips</strong></p><ul><li>about branch<ul><li>master<ul><li>pros: compatible to Mshot (CVPR22), relatively easy to install</li><li>cons: out-dated, hard-coded</li></ul></li><li>v1.1<ul><li>pros: fancy code and demos</li><li>cons: OpenGL and OSMesa would be a little bit tricky, not compatible to Mshot repo.<ul><li><ol><li>output *.pkl file should be slightly changed</li></ol></li><li><ol start="2"><li>this demo code doesn't generate instance segmentation mask per person, which is necessary to Mshot preprocessing.</li></ol></li></ul></li></ul></li></ul></li><li>My settings<ul><li>docker container / without conda</li><li>off-screen<ul><li>Pyrenderer and OpenGL may cause problems, you need to choose EGL or OSMesa</li><li>I build OSMesa and use PyOpenGL + OSMesa combination. (check 'Failure Logs...' under)</li></ul></li><li>works on PHALP master and v1.1 branches, and also work for Mshot.</li><li>Here is pip list output of my environment. <a href="https://github.com/sghong977/sghong977.github.io/files/11197975/req.txt" target="_blank" rel="noopener noreferrer">req.txt</a></li></ul></li></ul><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> Failure Logs... (skip this) </summary><div><div class="collapsibleContent_i85q"><p>Logs</p><ul><li>follow this <a href="https://github.com/brjathu/PHALP#installation" target="_blank" rel="noopener noreferrer">PHALP</a></li><li>it takes quite a time to solve environment in conda...</li><li>"('Connection broken: OSError("(104, \'ECONNRESET\')")', OSError("(104, 'ECONNRESET')"))" error</li><li>I'm using docker container<ul><li>to install git+http, 'apt-get install git'</li><li>pycocotools error: <a href="https://stackoverflow.com/questions/72611914/error-could-not-build-wheels-for-pycocotools-which-is-required-to-install-pypr" target="_blank" rel="noopener noreferrer">link</a></li></ul></li><li>torch.ao is added on 1.10</li></ul><p>I tried setting an environment without conda, but failed. issues while installing PHALP:</p><ul><li>pip install pyrootutils submitit gdown dill colordict scenedetect pytube</li><li>pip install scikit-learn==0.22.2</li><li>OpenGL: ssh env (without display)<ul><li>install OSMesa <a href="https://pyrender.readthedocs.io/en/latest/install/index.html#installmesa" target="_blank" rel="noopener noreferrer">OSMesa doc</a></li><li>apt-get install libexpat1-dev</li></ul></li><li>encountered on PosixPath ~~~ error but I couldn't handle it</li></ul></div></div></details><hr><h1>2. OpenPose</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="settings">Settings<a href="#settings" class="hash-link" aria-label="Direct link to Settings" title="Direct link to Settings">​</a></h3><ul><li>docker image from the hub: 'd0ckaaa/openpose'</li><li>execute make commands to install</li><li>download pretrained opencv models by get models script.</li></ul><hr><h1>3. multishot human pose setup</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="settings-1">Settings<a href="#settings-1" class="hash-link" aria-label="Direct link to Settings" title="Direct link to Settings">​</a></h3><ul><li>follow this <a href="https://github.com/geopavlakos/multishot#installation-instructions" target="_blank" rel="noopener noreferrer">multishot</a></li><li>I use the same docker as the previous one.</li><li>Notes<ul><li>use exactly the same version of smplx (if not, it may cause error)</li><li>no need to use torch 1.5.</li><li>change environ variables: PYOPENGL_PLATFORM' egl → osmesa</li><li>expand docker's shared memory size to 2GB</li></ul></li><li>Data<ul><li>download mshot_data from github official Mshot page (approximately 1GB)</li><li>download smpl neutral pkl and gmm08 on the official smplify webpage and move to the correct location</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pre-processing">Pre-processing<a href="#pre-processing" class="hash-link" aria-label="Direct link to Pre-processing" title="Direct link to Pre-processing">​</a></h3><ul><li><em>.pkl → </em>.npz</li><li>output contains information for a specific person (create separately)</li><li>these are the initial value of optimization step.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Direct link to Optimization" title="Direct link to Optimization">​</a></h3><ul><li>optimize the SMPL parameters using multishot loss</li></ul><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary> Memo </summary><div><div class="collapsibleContent_i85q"><h3 class="anchor anchorWithStickyNavbar_LWe7" id="notes">Notes.<a href="#notes" class="hash-link" aria-label="Direct link to Notes." title="Direct link to Notes.">​</a></h3><ul><li>To extract SMPL by multishot human pose, tracking and identification are needed.</li><li>What about the other SMPL algorithms? SMPL extraction models like ROMP, VIBE...<ul><li>Do they use 2D keypoints?</li><li>Regression or optimized based?</li><li>How are they inputs like? sequence or only an image? should be cropped? do they work on multiple people?</li><li>VIBE: <a href="https://arxiv.org/pdf/1912.05656.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1912.05656.pdf</a></li><li></li></ul></li></ul></div></div></details>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="SMPL" term="SMPL"/>
        <category label="3D Human Pose" term="3D Human Pose"/>
        <category label="Pose Estimation" term="Pose Estimation"/>
        <category label="PHALP" term="PHALP"/>
        <category label="MultiShot" term="MultiShot"/>
        <category label="OpenPose" term="OpenPose"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[LERF; Language Embedded Radiance Fields]]></title>
        <id>https://github.com/ai-blog/blog/lerf</id>
        <link href="https://github.com/ai-blog/blog/lerf"/>
        <updated>2023-03-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[let's start!]]></summary>
        <content type="html"><![CDATA[<p>let's start!</p><p>LERF</p><ul><li>LERF can be reconstructed from a hand-held phone capture within 45 minutes</li><li>then can <strong>render dense relevancy maps</strong> <strong>given textual queries</strong> interactively in real-time</li><li><a href="https://www.lerf.io/" target="_blank" rel="noopener noreferrer">link</a></li><li>the immediate output of NeRFs is nothing but a colorful density field, devoid of meaning or context, which inhibits building interfaces for interacting with the resulting 3D scenes</li><li>why natural language<ul><li>handle natural language input queries</li><li>ability to incorporate semantics at multiple scales and relate to long-tail and abstract concepts</li></ul></li></ul><p>How?</p><ul><li>CLIP without finetuning</li><li>construct a LERF by optimizing a language field jointly with NeRF<ul><li>which takes both position and physical scale as input</li><li>and outputs a single CLIP vector</li><li>supervised</li><li>CLIP embeddings generated from image crops of training views -&gt; multi-scale feature pyramid</li></ul></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Paper Review" term="Paper Review"/>
        <category label="NeRF" term="NeRF"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trends on March 2023]]></title>
        <id>https://github.com/ai-blog/blog/2303</id>
        <link href="https://github.com/ai-blog/blog/2303"/>
        <updated>2023-03-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[AI-related things that I got on this week.]]></summary>
        <content type="html"><![CDATA[<p>AI-related things that I got on this week.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="blender-gpt">Blender GPT<a href="#blender-gpt" class="hash-link" aria-label="Direct link to Blender GPT" title="Direct link to Blender GPT">​</a></h2><ul><li>GPT-4 based natural language command for easy usage </li><li><a href="https://github.com/gd3kr/BlenderGPT" target="_blank" rel="noopener noreferrer">https://github.com/gd3kr/BlenderGPT</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lerf">LERF<a href="#lerf" class="hash-link" aria-label="Direct link to LERF" title="Direct link to LERF">​</a></h2><ul><li>"Language Embedded Radiance Fields"</li><li>from UC Berkeley, authors are Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik</li><li>this will be integrated to nerfstudio</li><li><a href="https://www.lerf.io/" target="_blank" rel="noopener noreferrer">https://www.lerf.io/</a><img loading="lazy" src="https://www.lerf.io/data/nerf_render.svg" alt="image" class="img_ev3q"></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Trends" term="Trends"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Briefly review the code of deep feature, DISK]]></title>
        <id>https://github.com/ai-blog/blog/disk</id>
        <link href="https://github.com/ai-blog/blog/disk"/>
        <updated>2023-03-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How can I custom COLMAP to use the other deep features? Currently I'm stuck into this, so I'll take a look at the way how DISK customize their output to COLMAP for the following SfM steps.]]></summary>
        <content type="html"><![CDATA[<p>How can I custom COLMAP to use the other deep features? Currently I'm stuck into this, so I'll take a look at the way how DISK customize their output to COLMAP for the following SfM steps.</p><p>These days, I'm jumping in the deep end to get familiar with COLMAP and basics of multiview geometry.
I'm trying to use deep features (eg. LoFTR) to COLMAP but failed while sparse reconstruction. I think it's because of my wrong usage of matches_importer.
And I find that Sitcoms3D do SfM using COLMAP and they also replace the basic SIFT features of colmap into 'DISK' feature.</p><p>So, things that I have to do in this step are:  </p><ul class="contains-task-list containsTaskList_mC6p"><li class="task-list-item"><input type="checkbox" checked="" disabled=""> <!-- -->briefly see the concept of DISK feature algorithm</li><li class="task-list-item"><input type="checkbox" checked="" disabled=""> <!-- -->input/output</li><li class="task-list-item"><input type="checkbox" checked="" disabled=""> <!-- -->how to save and load feature in COLMAP</li><li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->try to use DISK official code to check the correct values of colmap db sqlite commands</li><li class="task-list-item"><input type="checkbox" disabled=""> <!-- -->then, insert my custom feature algorithm to colmap db by editing sqlite commands</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="concept-of-disk-feature-algorithm">Concept of 'DISK' feature algorithm<a href="#concept-of-disk-feature-algorithm" class="hash-link" aria-label="Direct link to Concept of 'DISK' feature algorithm" title="Direct link to Concept of 'DISK' feature algorithm">​</a></h2><p>Paper: "DISK: Learning local features with policy gradient" <a href="https://arxiv.org/abs/2006.13566" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2006.13566</a> (NeurIPS 2020)</p><ul><li>first, extract a set of local features FA and FB from each</li><li>and then match them to produce a set of correspondences MA↔B</li><li>To learn how to do this through reinforcement learning, they probabilistically redefine feature distribution and match distribution.</li><li>about reward function &amp; gradient estimator are described in paper... (idk RL..)
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/227893163-6f41b6c9-05ce-4e59-82f2-92191ecb71c7.png" alt="image" class="img_ev3q"></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="inputoutput">Input/output<a href="#inputoutput" class="hash-link" aria-label="Direct link to Input/output" title="Direct link to Input/output">​</a></h2><p>setup 'DISK' <a href="https://github.com/cvlab-epfl/disk" target="_blank" rel="noopener noreferrer">link</a> environment and try.</p><ul><li>Although I'm using DISK nested in Sitcoms3D repo <a href="https://github.com/ethanweber/sitcoms3D/tree/master/external/disk." target="_blank" rel="noopener noreferrer">link</a> but it may not work. Better to use official code.<ul><li>i just manually download pretrained pth file, and replace detect.py into the official version.</li></ul></li><li>if you use docker, you may need to increase shared memory size. Default docker setting is 64MB, but out of shared memory problem occured to me, so I added '--shm-size=2gb'</li></ul><p><strong>run detect.py</strong></p><ul><li>INPUT: frames directory</li><li>OUTPUT: *.h5 file (descriptors and keypoints)</li></ul><p>Output shape and values of detect.py
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/228707697-843c2909-84c9-4474-92c0-feea96ea6d77.png" alt="image" class="img_ev3q"></p><p><strong>run match.py</strong></p><ul><li>INPUT: *.h5 file directory</li><li>OUTPUT: matches.h5 file (descriptors and keypoints)</li></ul><p><strong>run colmap/h5_to_db.py</strong></p><ul><li>INPUT: frame dir, h5 dir, output db file path (end with *.db)<ul><li>but Sitcoms3D and my case need masking, so I use binary mask that generated from Detectrion2 (Mask-RCNN)</li></ul></li><li>OUTPUT: db file that compatible to COLMAP</li></ul><p>check values of db add_matches()'s params</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/228710921-6f84ac5d-19ab-4652-a4b3-84b6557cadac.png" alt="image" class="img_ev3q"></p><p>array to blob. Original array contains pair information of matching, idx1 and idx2 of each image's keypoint id.
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/228712036-4a099000-f426-46b4-9bbf-1fc439932b77.png" alt="image" class="img_ev3q"></p><p>Then, use exhaustive_matcher and mapper for sparse reconstruction.
These all works well in my case.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-save-and-load-feature-in-colmap">How to save and load feature in COLMAP.<a href="#how-to-save-and-load-feature-in-colmap" class="hash-link" aria-label="Direct link to How to save and load feature in COLMAP." title="Direct link to How to save and load feature in COLMAP.">​</a></h2><p>Please refer to 'DISK' repo.</p><p>Step 1. Feature Extraction.</p><ul><li>python detect.py h5_artifacts_destination images_directory</li><li>eg. python <strong>detect.py</strong> --height 1024 --width 1024 --n 2048 scene/h5 scene/images</li></ul><p>Step 2. Match Keypoints</p><ul><li>python match.py h5_artifacts_destination</li><li>eg. python <strong>match.py</strong> --rt 0.95 --save-threshold 100 scene/h5</li></ul><p>Step 3. Convert to COLMAP *.db file</p><ul><li>features are inserted to db <strong>without descriptor</strong>.</li><li>eg. python colmap/<strong>h5_to_db.py</strong> --database-path scene/database.db scene/h5 scene/images</li></ul><p>Step 4. execute colmap cli commands</p><ul><li><strong>exhaustive_matcher</strong> and <strong>mapper</strong> (sparse reconstruction)</li></ul><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/227842755-98eef01c-58f4-4dfe-8b62-b45adf6b03b8.png" alt="image" class="img_ev3q"></p><p>Then, let's check how to convert h5 to db file and insert my custom features like that.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-kinds-of-information-should-be-added-to-db-file">What kinds of information should be added to *.db file?<a href="#what-kinds-of-information-should-be-added-to-db-file" class="hash-link" aria-label="Direct link to What kinds of information should be added to *.db file?" title="Direct link to What kinds of information should be added to *.db file?">​</a></h3><p>h5_to_db.py do this.
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/227880414-0060bb63-d1b1-4090-840c-d09431cfc80c.png" alt="image" class="img_ev3q">{:width="80%" height="80%"}</p><ul><li><strong>db.create_tables()</strong><ul><li>define tables that conpatible to colmap usage</li><li>by executing 'CREATE_{CAMERAS, IMAGES, KEYPOINTS, DESCRIPTORS, MATCHES, TWO_VIEW_GEOMETRIES, NAME_INDEX}_TABLE' which are defined in colmap/colmap/database.py</li></ul></li><li><strong>add_keypoints()</strong><ul><li>this method() returns 'fname_to_id' (fname_to_id<!-- -->[filename]<!-- --> = image_id)<ul><li>connected to db.add_keypoints(image_id, keypoints) which are defined in colmap/colmap/database.py</li></ul></li><li>(optional) create_camera(): add new camera model and intrinsics</li></ul></li><li><strong>check_masking(h5_path, mask_path)</strong><ul><li>this method() returns 'mask_keyp_id' (mask_keyp_id<!-- -->[filename]<!-- --> = keep_inds)</li><li>read mask files and check if keypoint is valid or not, and keep this information in 'mask_keyp_id'.</li></ul></li><li><strong>add_matches(db, h5_path, fname_to_id, mask_keyp_id)</strong><ul><li>get colmap image ids (id_1, id_2) from fname_to_id</li><li>it calls the method db.add_matches(id_1, id_2, matches.T<!-- -->[masked_matches]<!-- -->) for image pairs, and add masked matching keypoints<ul><li>matches = group<!-- -->[key_2][()]</li><li>i think i need to check the value. is it scaled or not, or something else...</li><li><strong>my custom feature pairs should fit this form!</strong></li></ul></li></ul></li><li><strong>db.commit()</strong>: just commit the data to the *.db file</li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="DISK" term="DISK"/>
        <category label="Deep Features" term="Deep Features"/>
        <category label="COLMAP" term="COLMAP"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Trial; Scene Text Recognition (STR)]]></title>
        <id>https://github.com/ai-blog/blog/ocr</id>
        <link href="https://github.com/ai-blog/blog/ocr"/>
        <updated>2022-10-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Trying OCR for UI text recognition.]]></summary>
        <content type="html"><![CDATA[<p>Trying OCR for UI text recognition.</p><h1>Goal</h1><ul><li>Reason for trying <strong>parseq</strong>: It is state-of-the-art (SOTA).</li><li>The parseq code uses <strong>lmdb</strong>.</li><li>Is it necessary? Investigate other OCR development tools...</li></ul><h1>mmOCR</h1><blockquote><p>pip3 install openmim<br>
<!-- -->mim install mmcv-full<br>
<!-- -->mim install mmdet<br>
<!-- -->git clone <a href="https://github.com/open-mmlab/mmocr.git" target="_blank" rel="noopener noreferrer">https://github.com/open-mmlab/mmocr.git</a></p></blockquote><p><code>python mmocr/utils/ocr.py demo/demo_text_ocr.jpg --print-result --imshow</code></p><p>After setup, when you input an image, it cuts out the text in a fixed position, recognizes it, and visualizes the result.
Of course, if you run ocr.py, it can also perform detection (localization), but in our case, the format is fixed, so it's better to handle it as a simple cropping preprocessing step.</p><ul><li>Surprisingly, <strong>RobustScanner</strong> and <strong>SAR</strong> work well.</li><li>Naturally, if the text is split into two lines, it fails.</li></ul><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/193741326-972c698c-881a-4992-930f-823b01d21a09.png" alt="image" class="img_ev3q"></p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="OCR" term="OCR"/>
        <category label="STR" term="STR"/>
        <category label="Scene Text Recognition" term="Scene Text Recognition"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trends on September 2022]]></title>
        <id>https://github.com/ai-blog/blog/2209</id>
        <link href="https://github.com/ai-blog/blog/2209"/>
        <updated>2022-09-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[AI-related things that I got on this month.]]></summary>
        <content type="html"><![CDATA[<p>AI-related things that I got on this month.</p><p>As I was wrapping up a national project, I conducted some trend research. Although there are probably better free and paid websites for this, I referred to the following site. It provides recent Arxiv papers, popular tweets, and Reddit posts based on Twitter.</p><p>Deep Learning Monitor - Find new Arxiv papers, tweets and Reddit posts for you</p><ul><li><a href="https://deeplearn.org/" target="_blank" rel="noopener noreferrer">DeepLearn: https://deeplearn.org/</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-mlops">1. MLOps<a href="#1-mlops" class="hash-link" aria-label="Direct link to 1. MLOps" title="Direct link to 1. MLOps">​</a></h3><p><a href="https://arxiv.org/pdf/2209.09125v1.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2209.09125v1.pdf</a>  </p><p>Recently, there have been many stages involved in development and deployment. So the author conducted interviews with several individuals to write a paper on various aspects ranging from data collection to model versioning and utilization. They interviewed 18 engineers from both large and small companies in various fields such as autonomous driving and finance. As of the current time, this paper is ranked first in popularity based on the past month.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-transfer-based-methods--stylegan">2. transfer-based methods + StyleGAN<a href="#2-transfer-based-methods--stylegan" class="hash-link" aria-label="Direct link to 2. transfer-based methods + StyleGAN" title="Direct link to 2. transfer-based methods + StyleGAN">​</a></h3><p><a href="https://arxiv.org/pdf/2209.11224v1.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2209.11224v1.pdf</a>  </p><p>This paper combines <strong>image transfer-based methods and Style GAN</strong> to take both advantages.</p><ul><li>StyleGAN extracts <strong>style codes</strong> to create transformed images, but it has limitations due to fixed-crop.</li><li>On the other hand, image-to-image translation learns source-to-target mappings in a paired manner. Due to the paired supervision, it is difficult to achieve the same level of freedom in transformation as StyleGAN.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-neural-marker">3. neural marker<a href="#3-neural-marker" class="hash-link" aria-label="Direct link to 3. neural marker" title="Direct link to 3. neural marker">​</a></h3><p>This paper also focuses on video editing, similar to the second paper. It seems promising for advertising purposes.</p><p><a href="https://arxiv.org/pdf/2209.08896v1.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2209.08896v1.pdf</a></p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/192956177-c9bc04a2-e169-4ab9-b7bb-a05641bfda08.png" alt="image" class="img_ev3q"></p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Trends" term="Trends"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond to Action Recognition; A Survey on Advanced Video Understanding]]></title>
        <id>https://github.com/ai-blog/blog/action</id>
        <link href="https://github.com/ai-blog/blog/action"/>
        <updated>2022-08-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Goal of this post is to investigate Action Detection datasets such as AVA Action and AVA-Kinetics, as well as the Homage Dataset.]]></summary>
        <content type="html"><![CDATA[<p>Goal of this post is to investigate <strong>Action Detection</strong> datasets such as AVA Action and AVA-Kinetics, as well as the Homage Dataset.</p><h1>내용</h1><ol><li>Explore ActivityNet Challenges</li></ol><ul><li>Conducted initial research to grasp the trends by examining various datasets and tasks related to video understanding.</li><li>findings: I Decided to look into <strong>AVA-Kinetics and Homage</strong>.</li></ul><p>By the way, since 2019 that SlowFast won ActivityNet Challenges, Chinese groups have been consistently securing the 1st place in 20, 21, and 22.</p><ul><li>This year (2022) focuses on active speaker detection, which is different due to the use of audio.</li><li>1st place: <a href="https://unicon-asd.github.io/" target="_blank" rel="noopener noreferrer">Project Page</a>, Code to be released.</li><li>2nd place by Intel: <a href="https://arxiv.org/pdf/2112.01479.pdf" target="_blank" rel="noopener noreferrer">Paper</a></li></ul><ol start="2"><li>About AVA-Kinetics</li></ol><ul><li>Spatio-Temporal Action Localization Task</li><li>Research on state-of-the-art (SOTA): <a href="https://paperswithcode.com/sota/spatio-temporal-action-localization-on-ava" target="_blank" rel="noopener noreferrer">paperswithcode</a>, <a href="http://activity-net.org/challenges/2022/tasks/guest_ava.html" target="_blank" rel="noopener noreferrer">List of AVA Challenge Winners</a></li><li>"RM" outperformed the ACAR-Net (CVPR 2021 paper) which is a winning model of AVA Action (ActivityNet Challenge 2020)<ul><li>RM: This model won the AVA-Kinetics 2021 Challenge. <a href="https://arxiv.org/pdf/2106.08061v2.pdf" target="_blank" rel="noopener noreferrer">paper</a>, but the GitHub code is not available.</li></ul></li></ul><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/187596772-d0f79cc1-7b11-4586-bc10-5b0ea1ffa58a.png" alt="image" class="img_ev3q"></p><ol start="3"><li>About Homage</li></ol><p><strong>About Model</strong></p><ul><li>It seems meaningless to search for SOTA on paperswithcode as there are no notable models reported.</li><li>Official GitHub page provides detailed information about the dataset <a href="https://github.com/nishantrai18/homage" target="_blank" rel="noopener noreferrer">github</a></li><li>Let's check the challenge results. <a href="https://homeactiongenome.org/results.html" target="_blank" rel="noopener noreferrer">2021 Results</a></li><li>The 2022 results seem unavailable as the challenge ended in June. Challenge Page on <a href="https://codalab.lisn.upsaclay.fr/competitions/4264#results" target="_blank" rel="noopener noreferrer">CodaLab</a></li></ul><p><strong>About Data</strong></p><ul><li>Dataset Paper <a href="https://arxiv.org/pdf/2105.05226.pdf" target="_blank" rel="noopener noreferrer">link</a><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/187603295-1125faef-9a98-4c42-ad85-56f745269e55.png" alt="image" class="img_ev3q"></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="my-notes">My notes...<a href="#my-notes" class="hash-link" aria-label="Direct link to My notes..." title="Direct link to My notes...">​</a></h3><ul><li><a href="https://ambitious-posong.tistory.com/121" target="_blank" rel="noopener noreferrer">my blog</a></li><li>During the survey, I came across this: MIGS (BMVC 2021) <a href="https://github.com/migs2021/migs" target="_blank" rel="noopener noreferrer">GitHub</a>.<ul><li>Although it doesn't use the Homage dataset, it is listed on paperswithcode.</li></ul></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Survey" term="Survey"/>
        <category label="Action Recognition" term="Action Recognition"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[mmdetection; change scheduler, ensemble]]></title>
        <id>https://github.com/ai-blog/blog/detection5</id>
        <link href="https://github.com/ai-blog/blog/detection5"/>
        <updated>2022-08-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 5 for my VIPrior challenge journey link!]]></summary>
        <content type="html"><![CDATA[<p>Episode 5 for my VIPrior challenge journey <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>!</p><h1>Issue 1. change scheduler</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="using-this">Using this<a href="#using-this" class="hash-link" aria-label="Direct link to Using this" title="Direct link to Using this">​</a></h3><p>mmclassification doc. <a href="https://mmclassification.readthedocs.io/en/master/_modules/mmcls/core/hook/lr_updater.html#CosineAnnealingCooldownLrUpdaterHook" target="_blank" rel="noopener noreferrer">link</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="error">Error<a href="#error" class="hash-link" aria-label="Direct link to Error" title="Direct link to Error">​</a></h3><p>Error Message</p><blockquote><p>../aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: <!-- -->[176,0,0]<!-- -->, thread: <!-- -->[108,0,0]<!-- --> Assertion <!-- -->`<!-- -->input_val &gt;= zero &amp;&amp; input_val &lt;= one<!-- -->`<!-- --> failed.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="debug">Debug<a href="#debug" class="hash-link" aria-label="Direct link to Debug" title="Direct link to Debug">​</a></h3><p>Seems like a loss scale issue due to lr, so I re-enabled auto_lr.</p><p>Reference: <a href="https://discuss.pytorch.org/t/assertion-input-val-zero-input-val-one-failed/107554/3" target="_blank" rel="noopener noreferrer">https://discuss.pytorch.org/t/assertion-input-val-zero-input-val-one-failed/107554/3</a></p><p>FYI: There is a feature to receive <strong>notifications via email or Slack</strong> in case of crashes or completion of training.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/187104173-d19d8404-3479-4caa-baa4-5c28d58db66b.png" alt="image" class="img_ev3q"></p><h1>Issue 2. Ensemble</h1><p>I investigated and summarized it on my blog: <a href="https://ambitious-posong.tistory.com/201" target="_blank" rel="noopener noreferrer">Link, kor</a></p><p>The tool provided there takes image inputs, but there is no need to perform inference several times. Therefore, I will receive JSON files and code for ensemble.</p><p>Requirements</p><ul><li>Reference Code: <a href="https://github.com/ZFTurbo/Weighted-Boxes-Fusion" target="_blank" rel="noopener noreferrer">https://github.com/ZFTurbo/Weighted-Boxes-Fusion</a></li><li>output1: /raid/sghong/Detection/mmdetection/work_dirs/yoloxx_0826/inferences/bbox.json</li><li>output2: /nas4/viprior/sbhong/yoloxx/mmcls/submission.json</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="situation">Situation<a href="#situation" class="hash-link" aria-label="Direct link to Situation" title="Direct link to Situation">​</a></h3><ul><li>need to provide only bbox, category id, and score to Weighted-Boxes-Fusion.</li><li>Perform ensemble for each image and save the results (the order should not change due to the submission format).</li><li>Consensus will be added, similar to preprocessing as described in my blog.</li></ul><p>Keep using my Docker environment used for running the detection. The only thing I need for these ensemble is:
<code>pip install ensemble-boxes</code>.</p><p>Implementation</p><ul><li>Using the ensemble-boxes module</li><li>Take multiple JSON inference results as input -&gt; perform ensemble -&gt; then write the results as JSON.</li><li>Follow the 2022 VIPriors Object Detection Challenge submission format.</li><li>Consensus or other preprocessing(?) will be implemented tomorrow....</li></ul><p>My code link: <a href="https://github.com/sghong977/Daily_AIML/blob/fc4eede51557f6870e70939c54f9db855a3f6741/Object_Detection/ensemble.py" target="_blank" rel="noopener noreferrer">https://github.com/sghong977/Daily_AIML/blob/fc4eede51557f6870e70939c54f9db855a3f6741/Object_Detection/ensemble.py</a></p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="VIPrior" term="VIPrior"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[mmdetection YOLOX; image encoder transfer learning]]></title>
        <id>https://github.com/ai-blog/blog/detection4</id>
        <link href="https://github.com/ai-blog/blog/detection4"/>
        <updated>2022-08-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 4 for my VIPrior challenge journey link!]]></summary>
        <content type="html"><![CDATA[<p>Episode 4 for my VIPrior challenge journey <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>!</p><h1>Situation</h1><p>Following the previous issues (refer to the last posts), I have successfully resolved minor data annotation bugs and retrained the image encoder without any more issues. Now, let's bring it back to <strong>mmdetection and train the model</strong>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a href="#experiments" class="hash-link" aria-label="Direct link to Experiments" title="Direct link to Experiments">​</a></h3><ul><li>Backbone: CSPDarkNet (initialized with classification training)</li><li>Detection model: YOLOX (one-stage model)</li><li>Weight and Bias logs: <a href="https://wandb.ai/sghong/MMDetection-tutorial?workspace=user-sghong" target="_blank" rel="noopener noreferrer">mmdetection</a>, <a href="https://wandb.ai/sghong/MMcls-challenge?workspace=user-sghong" target="_blank" rel="noopener noreferrer">mmclassification</a></li><li>The results will be available in 4 hours, so I will update later.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-settings">Training Settings<a href="#training-settings" class="hash-link" aria-label="Direct link to Training Settings" title="Direct link to Training Settings">​</a></h3><p>List of transfer learning attempts this time:</p><ul><li>YOLOX + CSPDarknet (CE loss)</li><li>YOLOX + CSPDarknet transfer learning</li><li>Cascade RCNN + ResNet50 (CE loss)</li><li>Cascade RCNN + CBNetV2 (CE loss): Currently encountering an error.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results?<a href="#results" class="hash-link" aria-label="Direct link to Results?" title="Direct link to Results?">​</a></h3><ul><li><strong>Augmentation</strong>: There was a significant difference between using YOLOXHSVRandomAug or not (mAP difference of 0.1).</li><li><strong>Weight initialization</strong>: Transfer learning with (supervised) classification task performs better than using self-supervised learning for the backbone. Both settings are better than a general random initialization backbone.</li><li><strong>Loss</strong>: There was no significant difference between using <strong>CE loss or Label Smooth Loss</strong> during backbone training.</li><li><strong>Bug</strong>: Encountering nan loss in a specific model while performing mmclassification (ResNet50).</li></ul><p>In any case, we achieved a validation mAP of 0.284 in 300 epochs.
Let's try ensemble.
Config: /raid/sghong/Detection/mmdetection/work_dirs/yoloxx_0826/yolox_cls_aug.py</p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="VIPrior" term="VIPrior"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using mmclassification; CSPDarkNet, YOLO series]]></title>
        <id>https://github.com/ai-blog/blog/detection3</id>
        <link href="https://github.com/ai-blog/blog/detection3"/>
        <updated>2022-08-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 3 for my VIPrior challenge journey link!]]></summary>
        <content type="html"><![CDATA[<p>Episode 3 for my VIPrior challenge journey <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>!</p><h1>Situation</h1><p>Let's improve the detection performance through <strong>transfer learning of the backbone</strong>.
So, I need to switch to mmclassification and train the backbone, but it should be compatible with YOLOX in the existing mmdetection, so I need to train <strong>CSPDarkNet</strong>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cspnet-cross-stage-partial-network">CSPNet (Cross Stage Partial Network)<a href="#cspnet-cross-stage-partial-network" class="hash-link" aria-label="Direct link to CSPNet (Cross Stage Partial Network)" title="Direct link to CSPNet (Cross Stage Partial Network)">​</a></h3><ul><li>CSPNet enables efficient learning by removing gradient computation redundancy, without modifying the existing architecture.</li><li>Paper <a href="https://arxiv.org/pdf/1911.11929.pdf" target="_blank" rel="noopener noreferrer">Link</a></li></ul><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186355214-99922fe0-07eb-4b5b-927e-c2338fff22c8.png" alt="image" class="img_ev3q"></p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186355854-29e6fa22-99d7-4f92-a484-c1207bf6cb3b.png" alt="image" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lets-train">Let's train!<a href="#lets-train" class="hash-link" aria-label="Direct link to Let's train!" title="Direct link to Let's train!">​</a></h3><p>Code modification: Since CSPNet exists but the DarkNet version is missing, I copied some code from mmdetection.</p><ul><li>Just the basic version</li><li>Label Smooth</li><li>Multiple losses are not commonly used in classification. Refer to mmcls issue reply. <a href="https://github.com/open-mmlab/mmclassification/issues/796#issuecomment-1111715069" target="_blank" rel="noopener noreferrer">link</a></li><li>Training results: my mmcls wandb training logs. <a href="https://wandb.ai/sghong/MMcls-challenge" target="_blank" rel="noopener noreferrer">log</a></li><li>However, there was an small mistake when converting the data to coco format, so retraining is required...</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="miscellaneous">Miscellaneous<a href="#miscellaneous" class="hash-link" aria-label="Direct link to Miscellaneous" title="Direct link to Miscellaneous">​</a></h3><p>Joined the detection team in the challenge in a rush, lacking basic skills (this is my first time doing detection).
After the rush is over, I feel that I need to take time to read about the YOLO series...</p><p>Materials to read:</p><ul><li>A helpful article explaining CSPNet: <a href="https://keyog.tistory.com/30" target="_blank" rel="noopener noreferrer">Link</a></li><li>DarkNet, YOLO: <a href="http://supremastudy.blogspot.com/2018/11/yolo.html" target="_blank" rel="noopener noreferrer">Link</a></li><li>Comparison of the YOLO series: <a href="https://leedakyeong.tistory.com/entry/Object-Detection-YOLO-v1v6-%EB%B9%84%EA%B5%902" target="_blank" rel="noopener noreferrer">Link</a></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="VIPrior" term="VIPrior"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simply making Gitblog for accepted paper]]></title>
        <id>https://github.com/ai-blog/blog/gitblog</id>
        <link href="https://github.com/ai-blog/blog/gitblog"/>
        <updated>2022-08-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[For the camera-ready submission, I re-uploaded the code.]]></summary>
        <content type="html"><![CDATA[<p>For the camera-ready submission, I re-uploaded the code.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optional-creating-an-organization">[optional]<!-- --> Creating an Organization<a href="#optional-creating-an-organization" class="hash-link" aria-label="Direct link to optional-creating-an-organization" title="Direct link to optional-creating-an-organization">​</a></h2><p>Creating an Organization allows accessing it at <code>https://[organization-name].github.io/</code>.
You can upload repositories within your organization, but I skipped this step due to laziness.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186045015-d06d520a-ae77-4e15-b09f-d593b79cd4f1.png" alt="image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="uploading-the-code">Uploading the Code<a href="#uploading-the-code" class="hash-link" aria-label="Direct link to Uploading the Code" title="Direct link to Uploading the Code">​</a></h2><p>First, I upload the code that I want to make public.
I created a new repository: <a href="https://github.com/sghong977/Surgical-Bleeding-AMAGI.git" target="_blank" rel="noopener noreferrer">https://github.com/sghong977/Surgical-Bleeding-AMAGI.git</a></p><p><img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186047734-c3d69637-5a30-448b-99fa-8222dbdab69b.png" alt="image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="creating-a-git-blog">Creating a Git Blog<a href="#creating-a-git-blog" class="hash-link" aria-label="Direct link to Creating a Git Blog" title="Direct link to Creating a Git Blog">​</a></h2><ul><li>Reference: GitHub Pages <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">link</a></li><li>After creating the repository, I added a template and made modifications in vscode before pushing.</li><li>By installing the Markdown Preview Extension, it's convenient to make edits.</li></ul><p>Github page link for my MICCAI 2022 workshop paper! <a href="https://sghong977.github.io/bleeding/" target="_blank" rel="noopener noreferrer">https://sghong977.github.io/bleeding/</a></p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Debug" term="Debug"/>
        <category label="Paper Writing" term="Paper Writing"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rebuttal format for my workshop paper]]></title>
        <id>https://github.com/ai-blog/blog/rebuttal</id>
        <link href="https://github.com/ai-blog/blog/rebuttal"/>
        <updated>2022-08-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Currently preparing the camera-ready version for the MICCAI Workshop paper.]]></summary>
        <content type="html"><![CDATA[<p>Currently preparing the camera-ready version for the MICCAI Workshop paper.
Although it's a workshop paper, the proceedings are published as a journal, so they have journal-like requirements.</p><ul><li><p>Template used this time: <a href="https://zenkelab.org/resources/latex-rebuttal-response-to-reviewers-template/" target="_blank" rel="noopener noreferrer">link</a>. It's simple but beautiful.</p></li><li><p>This one is also neat and feels very journal-like. It's different from writing a Rebuttal for conferences like CVPR... <a href="https://github.com/mschroen/review_response_letter" target="_blank" rel="noopener noreferrer">link</a></p></li></ul>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Debug" term="Debug"/>
        <category label="Paper Writing" term="Paper Writing"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[VIPrior challenge; Our Backbone, Roboflow]]></title>
        <id>https://github.com/ai-blog/blog/detection2</id>
        <link href="https://github.com/ai-blog/blog/detection2"/>
        <updated>2022-08-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 2 for my VIPrior challenge journey link!]]></summary>
        <content type="html"><![CDATA[<p>Episode 2 for my VIPrior challenge journey <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>!</p><h1>Issue 1</h1><p>Using mmclassification / Creating Object Detection Backbone</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="goal-improving-object-detection-performance">Goal: Improving Object Detection Performance<a href="#goal-improving-object-detection-performance" class="hash-link" aria-label="Direct link to Goal: Improving Object Detection Performance" title="Direct link to Goal: Improving Object Detection Performance">​</a></h3><ol><li>Generate datasets by cropping images using bounding boxes (using roboflow)</li><li>Train an image classification pretrained model (image encoder) using mmclassification</li><li>Load the well-trained image encoder and use it for training our detection model</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tool-mmclassification">Tool: mmclassification<a href="#tool-mmclassification" class="hash-link" aria-label="Direct link to Tool: mmclassification" title="Direct link to Tool: mmclassification">​</a></h3><p>To ensure compatibility with our current setting (mmdetection), planning to use mmclassification for training.</p><ul><li><a href="https://github.com/open-mmlab/mmclassification" target="_blank" rel="noopener noreferrer">mmclassification</a></li><li>mmcls <a href="https://mmclassification.readthedocs.io/en/latest/getting_started.html" target="_blank" rel="noopener noreferrer">docx</a></li></ul><hr><h1>Issue 2</h1><p>Using Roboflow</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-versioning">Data Versioning<a href="#data-versioning" class="hash-link" aria-label="Direct link to Data Versioning" title="Direct link to Data Versioning">​</a></h3><ul><li>Goal: Pretrain the image encoder to improve performance.</li><li>Approach: Crop images within the bounding boxes and train for classification.</li><li>Method: Go to "Versions" in roboflow -&gt; Select the dataset -&gt; Choose "isolate objects" in preprocessing -&gt; then Dataset is generated!</li><li>Result: A new version of the dataset has been created.</li></ul><p>Old Dataset
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186087436-e585a3cd-b621-4c8c-b660-01c7f84372fd.png" alt="image" class="img_ev3q"></p><p>New Dataset
<img loading="lazy" src="https://user-images.githubusercontent.com/46152199/186086783-e6408c69-2aa5-4dd8-ad17-caba034a358c.png" alt="image" class="img_ev3q"></p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="VIPrior" term="VIPrior"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to improve Object Detection performance? + tools]]></title>
        <id>https://github.com/ai-blog/blog/detection</id>
        <link href="https://github.com/ai-blog/blog/detection"/>
        <updated>2022-08-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Episode 1 for my VIPrior challenge journey link!]]></summary>
        <content type="html"><![CDATA[<p>Episode 1 for my VIPrior challenge journey <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>!</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="useful-papers">Useful Papers<a href="#useful-papers" class="hash-link" aria-label="Direct link to Useful Papers" title="Direct link to Useful Papers">​</a></h3><ol><li>Making Images Real Again: A Comprehensive Survey on Deep Image Composition</li><li>Bag of Freebies for Training Object Detection Neural Networks</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-composition">Image Composition?<a href="#image-composition" class="hash-link" aria-label="Direct link to Image Composition?" title="Direct link to Image Composition?">​</a></h3><ul><li>The issue of <strong>small datasets</strong> -&gt; The concept of image composition came up as a way to diversify information through <strong>data augmentation</strong>.</li><li>(Refer to Paper 1) It was shared by a team member that image compositing is not about randomly placing objects like simple copy &amp; paste algorithm. Instead, it aims to make the composition <strong>more realistic</strong>.<ul><li>For example, human should be not copy&amp;pasted in the sky, since we cannot fly!</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tools">Tools<a href="#tools" class="hash-link" aria-label="Direct link to Tools" title="Direct link to Tools">​</a></h3><ul><li>I came across a website called <a href="https://voxel51.com/docs/fiftyone/" target="_blank" rel="noopener noreferrer">FiftyOne</a>. It seems to be related to detection bounding boxes and instance segmentation annotation tools. I should take a look at it.</li><li><a href="https://roboflow.com/" target="_blank" rel="noopener noreferrer">Roboflow</a>: It provides detailed analysis of the dataset itself, various augmentations, and the ability to train models on the web.<ul><li>The visualization is also well-done, making it convenient.</li><li>It seems that training is free for up to 3 times, and an email will be sent when the training is completed.</li><li>I haven't used either of them yet, so I'll make a note after trying them out.<ul><li>-&gt; (UPDATED) I totally fall in love to the Roboflow!</li></ul></li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hyperparameter-tuning">Hyperparameter Tuning?<a href="#hyperparameter-tuning" class="hash-link" aria-label="Direct link to Hyperparameter Tuning?" title="Direct link to Hyperparameter Tuning?">​</a></h3><ul><li>Refer to <strong>Paper 2</strong>.</li><li>(My teammate who keep on this field told me that) It is recommended to <strong>use well-known hyperparameters</strong> as they are, instead of hyperparameter tuning and search.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="small-object-high-bounding-box-overlap-issue">Small Object, High Bounding Box Overlap Issue<a href="#small-object-high-bounding-box-overlap-issue" class="hash-link" aria-label="Direct link to Small Object, High Bounding Box Overlap Issue" title="Direct link to Small Object, High Bounding Box Overlap Issue">​</a></h3><p>When training for small objects, a simple method is to increase the resolution.
We had a meeting to analyze the characteristics of the current target data (VIPriors <a href="https://vipriors.github.io/" target="_blank" rel="noopener noreferrer">link</a>, for challenge) through the dataset health check in Roboflow and find possible solutions.</p><p>The discussion content, future improvements, and work direction will be added below after the meeting.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="idea-after-meeting">Idea? (after meeting)<a href="#idea-after-meeting" class="hash-link" aria-label="Direct link to Idea? (after meeting)" title="Direct link to Idea? (after meeting)">​</a></h3><ul><li>Train based on their own characteristics: Divide into small, medium, and large categories.<ul><li>for example, in the case of bicycle data where there are many overlapping small object bounding boxes, divide them into multiple models, each responsible for a few classes. Then, combine the results during inference.</li><li>It is not an ensemble, simply divide the recognizable classes by several models.</li></ul></li><li>Transfer learning: Pretrained models are not allowed in this challenge. But, instead of random initialization, what if we crop object bounding boxes and <strong>train an image classification model</strong> to <strong>initialize the image encoder</strong>?</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="etc">etc<a href="#etc" class="hash-link" aria-label="Direct link to etc" title="Direct link to etc">​</a></h3><p>survey on crowd detection...</p>]]></content>
        <author>
            <name>SeulGi Hong</name>
            <uri>https://github.com/sghong977</uri>
        </author>
        <category label="Research Note" term="Research Note"/>
        <category label="Object Detection" term="Object Detection"/>
        <category label="VIPrior" term="VIPrior"/>
    </entry>
</feed>