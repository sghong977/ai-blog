---
slug: trainE2FGVI
title: Reproduce E2FGVI
authors: seulgi
tags: [Research Note, Video Inpainting]
---

Currently I'm improving a object removal pipeline. Since new year came, estimating the time and device cost is crucial to build a intricate plan for rolling a ball successfully!
So, in this post, it would be a super easy task because only thing I need now is just reproducing E2FGVI to measure cost.
But I don't wanna repetatively seeking on the information I need tho... That's the reason why I write this.

## Task
- **Goal**
  - train E2FGVI for the furter works
  - I need to measure the training costs like time or GPU amount

## Step 1. Data Preparation
The official E2FGVI model is trained on Youtube-VOS only, following the official dataset splits.
First of all, sign up to CodaLab and download the dataset. It takes time because of the size, approximately 50GB!

### Youtube VOS?
For a video inpainting task, Youtube-VOS 2018 is one of the widely used datasets.
[Youtube VOS link](https://youtube-vos.org/dataset/vos/), [link](https://drive.google.com/drive/folders/1bI5J1H3mxsIGo7Kp-pPZU8i6rnykOw7f)

- Training: 3471 videos with dense (6 fps) object annotations, 65 categories and 5945 unique object instances.
- Validation: 474 videos, 65 training (seen) categories, 26 unseen categories and 894 unique object instances.
- Test: 508 videos, 65 training (seen) categories, 29 unseen categories, and 915 unique object instances.

What does it mean by 'categories' and 'unique object instance'?
I didn't know that because the papers only report average performance for every instance or only the comparison in the seen and unseen class.

The source of this dataset is Youtube-8M which is a video classification dataset.
The authors selected some categories and got the videos from Youtube8M.
- animals (e.g. ant, eagle, goldfish, person)
- vehicles (e.g. airplane, bicycle, boat, sedan)
- accessories (e.g. eyeglass, hat, bag)
- common objects (e.g. potted plant, knife, sign, umbrella)
- humans in various activities (e.g. tennis, skateboarding, motorcycling, surfing).

I think categories in the dataset are about a variance of objects, rather than analyzing in terms of object size, type, or difficulty level. I'm looking for the dataset that provides in-depth analysis... but I cannot see that available yet.

Q. Then, how to generate a segmentation mask? In an automatic way? or manually by human annotators?
- Each video clip is about 3âˆ¼6 seconds long and often contains multiple objects
- which are manually segmented by **professional annotators**.

Q. What does the data look like?
- for every clip, each object and appearing frame number is notated.
<img width="186" alt="image" src="https://github.com/sghong977/ai-blog/assets/46152199/6df88c9d-dc03-4641-b398-6a13a051d764">
<img width="1490" alt="image" src="https://github.com/sghong977/ai-blog/assets/46152199/e5ddf76c-17d6-4b5d-b925-41469a7efb08">


### VOST
- What is it? new dataset released by Youtube-VOS community, since ICCV 2023 challenge
- Task: Video object segmentation, on more challenging properties like dramatic changing
[link](https://www.vostdataset.org/)


## Step 2. Train!
Processing the dataset following the guide of E2FGVI github.
- use a full-frame version of the dataset. If not, it might occur an error
- 
