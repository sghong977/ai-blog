"use strict";(self.webpackChunkai_blog=self.webpackChunkai_blog||[]).push([[9225],{3905:(e,t,r)=>{r.d(t,{Zo:()=>s,kt:()=>b});var n=r(7294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var c=n.createContext({}),p=function(e){var t=n.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},s=function(e){var t=p(e.components);return n.createElement(c.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=p(r),g=o,b=m["".concat(c,".").concat(g)]||m[g]||u[g]||a;return r?n.createElement(b,i(i({ref:t},s),{},{components:r})):n.createElement(b,i({ref:t},s))}));function b(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,i=new Array(a);i[0]=g;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l[m]="string"==typeof e?e:o,i[1]=l;for(var p=2;p<a;p++)i[p]=r[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}g.displayName="MDXCreateElement"},4498:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var n=r(7462),o=(r(7294),r(3905));const a={slug:"ocr",title:"Simple Trial; Scene Text Recognition (STR)",authors:"seulgi",tags:["Research Note","OCR","STR","Scene Text Recognition"]},i=void 0,l={permalink:"/ai-blog/blog/ocr",source:"@site/blog/research-note/2022-10-04-ocr.md",title:"Simple Trial; Scene Text Recognition (STR)",description:"Trying OCR for UI text recognition.",date:"2022-10-04T00:00:00.000Z",formattedDate:"October 4, 2022",tags:[{label:"Research Note",permalink:"/ai-blog/blog/tags/research-note"},{label:"OCR",permalink:"/ai-blog/blog/tags/ocr"},{label:"STR",permalink:"/ai-blog/blog/tags/str"},{label:"Scene Text Recognition",permalink:"/ai-blog/blog/tags/scene-text-recognition"}],readingTime:.65,hasTruncateMarker:!0,authors:[{name:"SeulGi Hong",title:"Vision AI Engineer",url:"https://github.com/sghong977",imageURL:"https://avatars.githubusercontent.com/u/46152199?v=4",key:"seulgi"}],frontMatter:{slug:"ocr",title:"Simple Trial; Scene Text Recognition (STR)",authors:"seulgi",tags:["Research Note","OCR","STR","Scene Text Recognition"]},prevItem:{title:"Briefly review the code of deep feature, DISK",permalink:"/ai-blog/blog/disk"},nextItem:{title:"Trends on September 2022",permalink:"/ai-blog/blog/2209"}},c={authorsImageUrls:[void 0]},p=[],s={toc:p},m="wrapper";function u(e){let{components:t,...r}=e;return(0,o.kt)(m,(0,n.Z)({},s,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Trying OCR for UI text recognition."),(0,o.kt)("h1",{id:"goal"},"Goal"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Reason for trying ",(0,o.kt)("strong",{parentName:"li"},"parseq"),": It is state-of-the-art (SOTA)."),(0,o.kt)("li",{parentName:"ul"},"The parseq code uses ",(0,o.kt)("strong",{parentName:"li"},"lmdb"),"."),(0,o.kt)("li",{parentName:"ul"},"Is it necessary? Investigate other OCR development tools...")),(0,o.kt)("h1",{id:"mmocr"},"mmOCR"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"pip3 install openmim",(0,o.kt)("br",{parentName:"p"}),"\n","mim install mmcv-full",(0,o.kt)("br",{parentName:"p"}),"\n","mim install mmdet",(0,o.kt)("br",{parentName:"p"}),"\n","git clone ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/open-mmlab/mmocr.git"},"https://github.com/open-mmlab/mmocr.git"))),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"python mmocr/utils/ocr.py demo/demo_text_ocr.jpg --print-result --imshow")),(0,o.kt)("p",null,"After setup, when you input an image, it cuts out the text in a fixed position, recognizes it, and visualizes the result.\nOf course, if you run ocr.py, it can also perform detection (localization), but in our case, the format is fixed, so it's better to handle it as a simple cropping preprocessing step."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Surprisingly, ",(0,o.kt)("strong",{parentName:"li"},"RobustScanner")," and ",(0,o.kt)("strong",{parentName:"li"},"SAR")," work well."),(0,o.kt)("li",{parentName:"ul"},"Naturally, if the text is split into two lines, it fails.")),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/46152199/193741326-972c698c-881a-4992-930f-823b01d21a09.png",alt:"image"})))}u.isMDXComponent=!0}}]);