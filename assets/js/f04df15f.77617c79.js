"use strict";(self.webpackChunkai_blog=self.webpackChunkai_blog||[]).push([[9130],{5023:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"vid2avatar","metadata":{"permalink":"/ai-blog/blog/vid2avatar","source":"@site/blog/paper-reviews/2023-06-29-vid2avatar.md","title":"(REVIEW) Vid2Avatar, CVPR 2023","description":"Today, I\'m going to review the paper for this CVPR 2023, Vid2Avatar.","date":"2023-06-29T00:00:00.000Z","formattedDate":"June 29, 2023","tags":[{"label":"Paper Review","permalink":"/ai-blog/blog/tags/paper-review"},{"label":"NeRF","permalink":"/ai-blog/blog/tags/ne-rf"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"vid2avatar","title":"(REVIEW) Vid2Avatar, CVPR 2023","authors":"seulgi","tags":["Paper Review","NeRF"]},"nextItem":{"title":"Welcome to my blog!","permalink":"/ai-blog/blog/welcome"}},"content":"Today, I\'m going to review the paper for this CVPR 2023, Vid2Avatar.\\n\\n#### \\"Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition\\"\\n- ETH Z\xfcrich, CVPR 2023, [Project Page](https://moygcc.github.io/vid2avatar/), [Paper](https://files.ait.ethz.ch/projects/vid2avatar/main.pdf)\\n\\n# Summary\\n\\n\\n![image](http://files.ait.ethz.ch/projects/vid2avatar/assets/pipeline.png)\\n\\n- casting the segmentation and surface as a joint 3d optimization problem\\n  - external segmentation model X\\n- spherical space -> sample points are divided into in and out of the sphere\\n- inner space points -> canonical SDF -> resampling the points by making use of surface information -> canonical texture -> render RGB\\n- outer space points (randomly sampled) -> SDF (static scene, background) -> combined with dynamic foreground(human)"},{"id":"welcome","metadata":{"permalink":"/ai-blog/blog/welcome","source":"@site/blog/2023-06-28-welcome/index.mdx","title":"Welcome to my blog!","description":"Hi, welcome to my blog!","date":"2023-06-28T00:00:00.000Z","formattedDate":"June 28, 2023","tags":[{"label":"main","permalink":"/ai-blog/blog/tags/main"},{"label":"AI","permalink":"/ai-blog/blog/tags/ai"}],"readingTime":0.17,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"welcome","title":"Welcome to my blog!","authors":["seulgi"],"tags":["main","AI"]},"prevItem":{"title":"(REVIEW) Vid2Avatar, CVPR 2023","permalink":"/ai-blog/blog/vid2avatar"},"nextItem":{"title":"(MY NOTE) How to remove caption from images? - Language SAM","permalink":"/ai-blog/blog/captionSAM"}},"content":"Hi, welcome to my blog!\\n\\nimport MyLogo from \'/static/img/docusaurus.png\';\\n\\n<img src={MyLogo} width=\\"200\\"/>\\n\\n\\nI\'ll backup my posts in Tistory blog, my old gitblog, git issues (for survey), and other materials.\\nEverything would be placed here!"},{"id":"captionSAM","metadata":{"permalink":"/ai-blog/blog/captionSAM","source":"@site/blog/research-note/2023-06-23-captionSAM.md","title":"(MY NOTE) How to remove caption from images? - Language SAM","description":"How to remove caption from the video data?","date":"2023-06-23T00:00:00.000Z","formattedDate":"June 23, 2023","tags":[{"label":"Research Note","permalink":"/ai-blog/blog/tags/research-note"},{"label":"Segment Anything Model","permalink":"/ai-blog/blog/tags/segment-anything-model"},{"label":"Text Segmentation","permalink":"/ai-blog/blog/tags/text-segmentation"}],"readingTime":2.925,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"captionSAM","title":"(MY NOTE) How to remove caption from images? - Language SAM","authors":"seulgi","tags":["Research Note","Segment Anything Model","Text Segmentation"]},"prevItem":{"title":"Welcome to my blog!","permalink":"/ai-blog/blog/welcome"},"nextItem":{"title":"(TRENDS) 2023.04","permalink":"/ai-blog/blog/2304"}},"content":"How to remove caption from the video data?\\n\\n\\n## Task\\n- **Goal**\\n  - I want to remove caption area from images\\n  - Output sholud be like bbox, or non-rectangular mask only for the text area. Format doesn\'t matter but the later one is better to keep more information on remain scene\\n- **Issue**\\n  - prefer to use **generalized pretrained model**\\n  - Fully Automatic: without any supervision from human (without GUI prompt)\\n  - speed matters\\n\\n\\n## How?\\n**1. OCR**\\nFirst of all, optical character recognition would be a nice and trustful approach because it has been a long-lasting computer vision task.\\n- **Scene Text Detection (Localization)**: to get bbox area of the text in the image\\n  - e.g., CRAFT [https://github.com/clovaai/CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch) provides the pre-trained model\\n- Scene Text Recognition: I don\'t need this part\\n\\n**2. Generalized Segmentation Model**\\nThese days, \\"Segment Anything\\" is a trending topic in the field of computer vision. Therefore, I am examining the performance of SAM in masking text areas, and conducting a survey on follow-up studies.\\n\\n\\n## Step 1. What is SAM?\\nI believe the scene text detection task is well-known and has already achieved satisfactory performance. Therefore, I will explore how SAM works for my specific task.\\n\\n\\n- **Generalization**: SAM is general enough to cover a broad set of use cases and can be used out of the box on new image domains \u2014 whether underwater photos or cell microscopy \u2014 without requiring additional training (a capability often referred to as zero-shot transfer).\\n- **Time Cost**: using light-weight for the real-time inference\\n- **Promptable Segmentation Task**: take various prompts to train for enabling a valid segmentation even with the ambiguous inputs\\n  - e.g., Forground, Backgroud Points / Bounding Box / Masks / Free-form Text are defined as prompts.\\n  - But free-form text is not released (2023.04.06)\\n\\n\\n**How the model looks like:**\\n- To summarize, the model composed of 3 parts. image encoder, prompt encoder, and decoder.\\n- image encoder: MAE (Masked Auto-Encoder), takes 1024x1024 sized input, using pretrained ViT\\n- prompt encoder: the model convert any types of prompt to 256 dim.\\n  - text: using CLIP embedding\\n  - fg/bg or bbox: using 2 points (NN)\\n  - Mask: using CNN to reduce size and the last, 1*1 conv generates 256 dim code\\n![img](https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/338558258_1349701259095991_4358060436604292355_n.png?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=6YFtfSFN9TkAX9KWrZS&_nc_ht=scontent-gmp1-1.xx&oh=00_AfBir_KH9omTbT4lVU6Gx7XOIQ21vb_ytXcNu9ikPmO8XA&oe=649F0109)\\n\\n\\n**Dataset: SA-1B**\\n- COCO run-length encoding (RLE) annotation format\\n- provides 11M images and the huge amount of class agnostic, only for the research purpose (the mask doesn\'t contains the category!)\\n- masks are autogenerated by SAM\\n- you can request validation set, which is randomly sampled and annotated from human annotators.\\n\\n\\n**My comments:**\\nI need a fully automatic process but finding fg&bg or bbox would be an additional task. Luckly, I find a nice open-source project to tackle this!\\n\\n\\nHowever, Language SAM is not exactly the same as the original SAM architecture, since Language SAM is relying on text-to-bbox approach by combining SAM and GroundingDINO.\\n- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)\\n- [Segment-Anything](https://github.com/facebookresearch/segment-anything)\\n- GUI is available by [Lightning AI](https://github.com/Lightning-AI/lightning)! (but i didn\'t check this)\\n\\n\\n\\n## Step 2. Let\'s try Language SAM\\nLet\'s go back to our task, removing caption area.\\nLanguage segment anything model: [github link](https://github.com/luca-medeiros/lang-segment-anything)\\n\\n``` Bash\\npip install torch torchvision\\npip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git\\n```\\n\\n**Outputs**\\n- The output is consists of instances, and each of them contain **binary mask, bounding box, phrase, logit**.\\n![image](https://github.com/sghong977/sghong977.github.io/assets/46152199/39aacc96-cea3-4ec6-bb6e-b31f7fcf146e)\\n\\nI didn\'t upload the output due to the copyright issue, but I\'ll add some visualization later on.\\n\\n\\nReferences\\n- Language SAM [https://github.com/luca-medeiros/lang-segment-anything](https://github.com/luca-medeiros/lang-segment-anything)\\n- [https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/](https://lightning.ai/pages/community/lang-segment-anything-object-detection-and-segmentation-with-text-prompt/)\\n- facebook [post](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/?ref=blog.annotation-ai.com)\\n- (KR) Explanation about SAM details [https://blog.annotation-ai.com/segment-anything/](https://blog.annotation-ai.com/segment-anything/)"},{"id":"2304","metadata":{"permalink":"/ai-blog/blog/2304","source":"@site/blog/tech-trends/2023-04-11-2304.md","title":"(TRENDS) 2023.04","description":"AI-related things that I got on this week.","date":"2023-04-11T00:00:00.000Z","formattedDate":"April 11, 2023","tags":[{"label":"Trends","permalink":"/ai-blog/blog/tags/trends"}],"readingTime":0.325,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"2304","title":"(TRENDS) 2023.04","authors":"seulgi","tags":["Trends"]},"prevItem":{"title":"(MY NOTE) How to remove caption from images? - Language SAM","permalink":"/ai-blog/blog/captionSAM"},"nextItem":{"title":"(DEBUG) Settings","permalink":"/ai-blog/blog/settings"}},"content":"AI-related things that I got on this week.\\n\\n\\n## MonoAvatar\\n- Monocular RGB video -> volumetric photorealistic 3D avatar\\n- can be rendered with user-defined expression and viewpoint.\\n- CVPR 23, Google and Simon Fraser University [link](https://augmentedperception.github.io/monoavatar/)\\n\\n## Segment Anything\\n- [Facebook Research](https://github.com/facebookresearch/segment-anything)\\n- [kor explain. link](https://blog.annotation-ai.com/segment-anything/)\\n\\n\\n## 3D Object Rigging from a Single Image\\n- CVPR 19 [animate pic](https://developer.nvidia.com/blog/transforming-paintings-and-photos-into-animations-with-ai/)\\n- ECCV 22 [obj wakeup](https://kulbear.github.io/object-wakeup/)"},{"id":"settings","metadata":{"permalink":"/ai-blog/blog/settings","source":"@site/blog/settings/2023-04-07-settings.md","title":"(DEBUG) Settings","description":"Stressful Settings...","date":"2023-04-07T00:00:00.000Z","formattedDate":"April 7, 2023","tags":[{"label":"Installation","permalink":"/ai-blog/blog/tags/installation"}],"readingTime":2.44,"hasTruncateMarker":true,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"settings","title":"(DEBUG) Settings","authors":["seulgi"],"tags":["Installation"]},"prevItem":{"title":"(TRENDS) 2023.04","permalink":"/ai-blog/blog/2304"},"nextItem":{"title":"(MY NOTE) Multishot Human Pose","permalink":"/ai-blog/blog/mshot"}},"content":"Stressful Settings...\\n\\n\\n\x3c!--truncate--\x3e\\n\\n\\n\\n# Dependencies\\n- Detectron2\\n  - python 3.7 >=\\n  - torch 1.8 >=\\n- scikit-learn == 0.22\\n  - it takes a while to set\\n  - pip: Cython\\n  - apt-get: gcc, g++\\n    - [link](https://woowaa.net/entry/gcc-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0)\\n- official neural renderer is tensorflow version.\\n\\n\\n***\\n\\n# Errors\\n### \\"ImportError: libGL.so.1: cannot open shared object file: No such file or directory\\"\\n`apt-get update && apt-get install ffmpeg libsm6 libxext6  -y`\\n\\n### ImportError: (\'Unable to load OpenGL library\', \'OSMesa: cannot open shared object file: No such file or directory\', \'OSMesa\', None)\\n- install OSMesa\\n\\n### AttributeError: \'PosixPath\' object has no attribute \'path\'\\n- [link](https://stackoverflow.com/questions/59693174/attributeerror-posixpath-object-has-no-attribute-path)\\n- [link2](https://ryanking13.github.io/2018/05/22/pathlib.html)\\n- PosixPath? When I print os.name, \'posix\' returned.\\n\\n\\n***\\n\\n# Docker\\n\\n### How to delete &lt;none&gt; docker images?\\n\\n`docker rmi $(docker images -f \\"dangling=true\\" -q)`\\n\\nAfter I tried this, every &lt;none&gt; dockers are deleted.\\n\\n\\n***\\n\\n# Installation\\n\\n### OpenPose\\n- Install guide [link](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation/0_index.md#compiling-and-running-openpose-from-source), [link2](https://jjeamin.github.io/posts/openpose/)\\n- I\'m using Ubuntu server and don\'t have GUI\\n- git clone regressively.\\n- .... I just download openpose docker image. docker pull d0ckaaa/openpose\\n- cmake ..\\n- make -j4`nproc`\\n- how to use: https://viso.ai/deep-learning/openpose/\\n  \\n### OSMesa\\n- Try this. [link](https://pyrender.readthedocs.io/en/latest/install/index.html#installmesa)\\n- Before try this,\\n  - apt-get install wget\\n  - apt-get install pkg-config\\n  - apt-get install libexpat1-dev\\n- i installed in /usr/local\\n- MESA_HOME=/usr/local (+/lib?)\\n- issues\\n  - https://redstarhong.tistory.com/98 GL? PyOpenGL?\\n  - update \\"/opt/conda/lib/python3.9/site-packages/OpenGL/platform/egl.py\\"\\n  - apt-get install libosmesa6-dev\\n  - if PyOpenGL is failed, clone git and reinstall PyOpenGL.\\n  \\n### PHALP  \\nI failed while using conda yml file because:\\n- First, I installed conda inside of a docker container to keep experiments going on.\\n  - when I tried to create conda env, it takes more than an hour to solve dependencies, but it still doesn\'t finished.\\n- Then, I tried to install every commands on my own, neural renderer and cuda path problems occurred.\\n  - It was hard for me to find cuda path in conda env because someone said that each conda environment would only contain the parts of cuda\\n  - So I should export every paths of cuda related stuff, but when I searched cuda paths in my server computer, there are too many other settings, so I\'m not sure what to add\\n- These are the reasons why I didn\'t use conda\\n\\nI use these commands sequentially. After that, demo works well on my docker container.\\nOfficial PHALP\'s colab pip output: [phalp_colab.txt](https://github.com/sghong977/sghong977.github.io/files/11175914/phalp_colab.txt)  \\n  \\n! pip install pyrender  \\n! pip install opencv-python  \\n! pip install joblib  \\n! pip install cython  \\n! pip install scikit-learn==0.22  \\n! pip install scikit-image  \\n! pip install git+https://github.com/facebookresearch/detectron2.git  \\n! pip install git+https://github.com/brjathu/pytube.git  \\n! pip install git+https://github.com/brjathu/NMR.git  \\n! pip install chumpy  \\n! pip install ipython  \\n! pip install gdown  \\n! pip install dill  \\n! pip install scenedetect[opencv]  \\n  \\nBut if you use v1.1, additional packages are needed.\\n- Install OSMesa\\n- pip install smplx submitit rich pyrootutils colordict\\n- pip install -e .\\n- install hydra-core again (Posix error)\\n- python scripts/demo.py video.source=\\\\\'\\"https://www.youtube.com/watch?v=xEH_5T9jMVU\\"\\\\\'\\n  - I use osmesa, not egl\\n![image](https://user-images.githubusercontent.com/46152199/230560634-e9edb760-e566-4d5c-88c1-ea38c4cf3855.png)"},{"id":"mshot","metadata":{"permalink":"/ai-blog/blog/mshot","source":"@site/blog/research-note/2023-04-05-mshot.md","title":"(MY NOTE) Multishot Human Pose","description":"I\'m trying to extract SMPL parameters on video in a multishot setting. To this end, tracklets are necessary.","date":"2023-04-05T00:00:00.000Z","formattedDate":"April 5, 2023","tags":[{"label":"Research Note","permalink":"/ai-blog/blog/tags/research-note"},{"label":"SMPL","permalink":"/ai-blog/blog/tags/smpl"},{"label":"3D Human Pose","permalink":"/ai-blog/blog/tags/3-d-human-pose"},{"label":"Pose Estimation","permalink":"/ai-blog/blog/tags/pose-estimation"},{"label":"PHALP","permalink":"/ai-blog/blog/tags/phalp"},{"label":"MultiShot","permalink":"/ai-blog/blog/tags/multi-shot"},{"label":"OpenPose","permalink":"/ai-blog/blog/tags/open-pose"}],"readingTime":3.085,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"mshot","title":"(MY NOTE) Multishot Human Pose","authors":"seulgi","tags":["Research Note","SMPL","3D Human Pose","Pose Estimation","PHALP","MultiShot","OpenPose"]},"prevItem":{"title":"(DEBUG) Settings","permalink":"/ai-blog/blog/settings"},"nextItem":{"title":"(REVIEW) LERF; Language Embedded Radiance Fields","permalink":"/ai-blog/blog/lerf"}},"content":"I\'m trying to extract SMPL parameters on video in a multishot setting. To this end, tracklets are necessary.\\n\\n\\n### Steps: PHALP setup -> Multishot Human Pose extraction -> select data for training NeRF.\\n\\n\\n# 1. PHALP setup\\n\\n### Summary of PHALP\\n- \\"Tracking People by Predicting 3D Appearance, Location & Pose\\" (CVPR 2022)\\n- Input: monocular video\\n- Task: Tracking people\\n- Method\\n  - lift people in a frame to 3D (3D pose of human, human location in 3D space, 3D appearance)\\n  - track a person: collect 3D observations over time in a tracklet representation -> build temporal model\\n    - model predict the **future state of a tracklet including 3d location, pose, appearance**\\n    - for the future frame, calculate similarity between pred state of tracklet and single frame observations\\n  - Association: simple Hungarian matching\\n- Output: *.pkl file (input for the next step, multishot human pose extraction)\\n- project page: [PHALP project](http://people.eecs.berkeley.edu/~jathushan/PHALP/)\\n\\n\\n### Installation\\n**Tips**\\n- about branch\\n  - master\\n    - pros: compatible to Mshot (CVPR22), relatively easy to install\\n    - cons: out-dated, hard-coded\\n  - v1.1\\n    - pros: fancy code and demos\\n    - cons: OpenGL and OSMesa would be a little bit tricky, not compatible to Mshot repo.\\n      - 1. output *.pkl file should be slightly changed\\n      - 2. this demo code doesn\'t generate instance segmentation mask per person, which is necessary to Mshot preprocessing.\\n- My settings\\n  - docker container / without conda\\n  - off-screen\\n    - Pyrenderer and OpenGL may cause problems, you need to choose EGL or OSMesa\\n    - I build OSMesa and use PyOpenGL + OSMesa combination. (check \'Failure Logs...\' under)\\n  - works on PHALP master and v1.1 branches, and also work for Mshot.\\n  - Here is pip list output of my environment. [req.txt](https://github.com/sghong977/sghong977.github.io/files/11197975/req.txt)\\n\\n\\n<details>\\n<summary> Failure Logs... (skip this) </summary>\\n\\nLogs\\n- follow this [PHALP](https://github.com/brjathu/PHALP#installation)\\n- it takes quite a time to solve environment in conda...\\n- \\"(\'Connection broken: OSError(\\"(104, \\\\\'ECONNRESET\\\\\')\\")\', OSError(\\"(104, \'ECONNRESET\')\\"))\\" error\\n- I\'m using docker container\\n  - to install git+http, \'apt-get install git\'\\n  - pycocotools error: [link](https://stackoverflow.com/questions/72611914/error-could-not-build-wheels-for-pycocotools-which-is-required-to-install-pypr)\\n- torch.ao is added on 1.10\\n\\n  \\nI tried setting an environment without conda, but failed. issues while installing PHALP:\\n- pip install pyrootutils submitit gdown dill colordict scenedetect pytube\\n- pip install scikit-learn==0.22.2\\n- OpenGL: ssh env (without display)\\n  - install OSMesa [OSMesa doc](https://pyrender.readthedocs.io/en/latest/install/index.html#installmesa)\\n  - apt-get install libexpat1-dev\\n- encountered on PosixPath ~~~ error but I couldn\'t handle it\\n\\n</details>\\n\\n\\n---\\n\\n# 2. OpenPose\\n### Settings\\n- docker image from the hub: \'d0ckaaa/openpose\'\\n- execute make commands to install\\n- download pretrained opencv models by get models script.\\n\\n---\\n\\n# 3. multishot human pose setup\\n### Settings\\n- follow this [multishot](https://github.com/geopavlakos/multishot#installation-instructions)\\n- I use the same docker as the previous one.\\n- Notes\\n  - use exactly the same version of smplx (if not, it may cause error)\\n  - no need to use torch 1.5.\\n  - change environ variables: PYOPENGL_PLATFORM\' egl \u2192 osmesa\\n  - expand docker\'s shared memory size to 2GB\\n- Data\\n  - download mshot_data from github official Mshot page (approximately 1GB)\\n  - download smpl neutral pkl and gmm08 on the official smplify webpage and move to the correct location\\n\\n### Pre-processing\\n- *.pkl \u2192 *.npz\\n- output contains information for a specific person (create separately)\\n- these are the initial value of optimization step.\\n\\n\\n### Optimization\\n- optimize the SMPL parameters using multishot loss\\n\\n\\n\\n<details>\\n<summary> Memo </summary>\\n  \\n\\n### Notes.\\n- To extract SMPL by multishot human pose, tracking and identification are needed.\\n- What about the other SMPL algorithms? SMPL extraction models like ROMP, VIBE...\\n  - Do they use 2D keypoints?\\n  - Regression or optimized based?\\n  - How are they inputs like? sequence or only an image? should be cropped? do they work on multiple people?\\n  - VIBE: https://arxiv.org/pdf/1912.05656.pdf\\n  - \\n\\n\\n</details>"},{"id":"lerf","metadata":{"permalink":"/ai-blog/blog/lerf","source":"@site/blog/paper-reviews/2023-03-28-lerf.md","title":"(REVIEW) LERF; Language Embedded Radiance Fields","description":"let\'s start!","date":"2023-03-28T00:00:00.000Z","formattedDate":"March 28, 2023","tags":[{"label":"Paper Review","permalink":"/ai-blog/blog/tags/paper-review"},{"label":"NeRF","permalink":"/ai-blog/blog/tags/ne-rf"}],"readingTime":0.675,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"lerf","title":"(REVIEW) LERF; Language Embedded Radiance Fields","authors":"seulgi","tags":["Paper Review","NeRF"]},"prevItem":{"title":"(MY NOTE) Multishot Human Pose","permalink":"/ai-blog/blog/mshot"},"nextItem":{"title":"(TRENDS) 2023.03","permalink":"/ai-blog/blog/2303"}},"content":"let\'s start!\\n\\nLERF\\n- LERF can be reconstructed from a hand-held phone capture within 45 minutes\\n- then can **render dense relevancy maps** **given textual queries** interactively in real-time\\n- [link](https://www.lerf.io/)\\n- the immediate output of NeRFs is nothing but a colorful density field, devoid of meaning or context, which inhibits building interfaces for interacting with the resulting 3D scenes\\n- why natural language\\n  - handle natural language input queries\\n  - ability to incorporate semantics at multiple scales and relate to long-tail and abstract concepts\\n\\n\\nHow?\\n- CLIP without finetuning\\n- construct a LERF by optimizing a language field jointly with NeRF\\n  - which takes both position and physical scale as input\\n  - and outputs a single CLIP vector\\n  - supervised\\n  - CLIP embeddings generated from image crops of training views -> multi-scale feature pyramid"},{"id":"2303","metadata":{"permalink":"/ai-blog/blog/2303","source":"@site/blog/tech-trends/2023-03-27-2303.md","title":"(TRENDS) 2023.03","description":"AI-related things that I got on this week.","date":"2023-03-27T00:00:00.000Z","formattedDate":"March 27, 2023","tags":[{"label":"Trends","permalink":"/ai-blog/blog/tags/trends"}],"readingTime":0.285,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"2303","title":"(TRENDS) 2023.03","authors":"seulgi","tags":["Trends"]},"prevItem":{"title":"(REVIEW) LERF; Language Embedded Radiance Fields","permalink":"/ai-blog/blog/lerf"},"nextItem":{"title":"(MY NOTE) Briefly review the code of deep feature, DISK","permalink":"/ai-blog/blog/disk"}},"content":"AI-related things that I got on this week.\\n\\n\\n\\n## Blender GPT\\n- GPT-4 based natural language command for easy usage \\n- https://github.com/gd3kr/BlenderGPT\\n\\n\\n## LERF\\n- \\"Language Embedded Radiance Fields\\"\\n- from UC Berkeley, authors are Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik\\n- this will be integrated to nerfstudio\\n- https://www.lerf.io/\\n![image](https://www.lerf.io/data/nerf_render.svg)"},{"id":"disk","metadata":{"permalink":"/ai-blog/blog/disk","source":"@site/blog/research-note/2023-03-24-disk.md","title":"(MY NOTE) Briefly review the code of deep feature, DISK","description":"How can I custom COLMAP to use the other deep features? Currently I\'m stuck into this, so I\'ll take a look at the way how DISK customize their output to COLMAP for the following SfM steps.","date":"2023-03-24T00:00:00.000Z","formattedDate":"March 24, 2023","tags":[{"label":"Research Note","permalink":"/ai-blog/blog/tags/research-note"},{"label":"DISK","permalink":"/ai-blog/blog/tags/disk"},{"label":"Deep Features","permalink":"/ai-blog/blog/tags/deep-features"},{"label":"COLMAP","permalink":"/ai-blog/blog/tags/colmap"}],"readingTime":3.63,"hasTruncateMarker":false,"authors":[{"name":"SeulGi Hong","title":"Vision AI Engineer","url":"https://github.com/sghong977","imageURL":"https://avatars.githubusercontent.com/u/46152199?v=4","key":"seulgi"}],"frontMatter":{"slug":"disk","title":"(MY NOTE) Briefly review the code of deep feature, DISK","authors":"seulgi","tags":["Research Note","DISK","Deep Features","COLMAP"]},"prevItem":{"title":"(TRENDS) 2023.03","permalink":"/ai-blog/blog/2303"}},"content":"How can I custom COLMAP to use the other deep features? Currently I\'m stuck into this, so I\'ll take a look at the way how DISK customize their output to COLMAP for the following SfM steps.\\n\\n\\n\\nThese days, I\'m jumping in the deep end to get familiar with COLMAP and basics of multiview geometry.\\nI\'m trying to use deep features (eg. LoFTR) to COLMAP but failed while sparse reconstruction. I think it\'s because of my wrong usage of matches_importer.\\nAnd I find that Sitcoms3D do SfM using COLMAP and they also replace the basic SIFT features of colmap into \'DISK\' feature.\\n\\nSo, things that I have to do in this step are:  \\n- [x] briefly see the concept of DISK feature algorithm\\n- [x] input/output\\n- [x] how to save and load feature in COLMAP\\n- [ ] try to use DISK official code to check the correct values of colmap db sqlite commands\\n- [ ] then, insert my custom feature algorithm to colmap db by editing sqlite commands\\n\\n\\n## Concept of \'DISK\' feature algorithm\\nPaper: \\"DISK: Learning local features with policy gradient\\" https://arxiv.org/abs/2006.13566 (NeurIPS 2020)\\n- first, extract a set of local features FA and FB from each\\n- and then match them to produce a set of correspondences MA\u2194B\\n- To learn how to do this through reinforcement learning, they probabilistically redefine feature distribution and match distribution.\\n- about reward function & gradient estimator are described in paper... (idk RL..)\\n![image](https://user-images.githubusercontent.com/46152199/227893163-6f41b6c9-05ce-4e59-82f2-92191ecb71c7.png)\\n\\n## Input/output\\nsetup \'DISK\' [link](https://github.com/cvlab-epfl/disk) environment and try.\\n- Although I\'m using DISK nested in Sitcoms3D repo [link](https://github.com/ethanweber/sitcoms3D/tree/master/external/disk.) but it may not work. Better to use official code.\\n  - i just manually download pretrained pth file, and replace detect.py into the official version.\\n- if you use docker, you may need to increase shared memory size. Default docker setting is 64MB, but out of shared memory problem occured to me, so I added \'--shm-size=2gb\'\\n\\n\\n**run detect.py**\\n- INPUT: frames directory\\n- OUTPUT: *.h5 file (descriptors and keypoints)\\n\\nOutput shape and values of detect.py\\n![image](https://user-images.githubusercontent.com/46152199/228707697-843c2909-84c9-4474-92c0-feea96ea6d77.png)\\n\x3c!-- <img width=\\"932\\" alt=\\"image\\" src=\\"https://user-images.githubusercontent.com/46152199/228707697-843c2909-84c9-4474-92c0-feea96ea6d77.png\\"> --\x3e\\n \\n**run match.py**\\n- INPUT: *.h5 file directory\\n- OUTPUT: matches.h5 file (descriptors and keypoints)\\n\\n**run colmap/h5_to_db.py**\\n- INPUT: frame dir, h5 dir, output db file path (end with *.db)\\n  - but Sitcoms3D and my case need masking, so I use binary mask that generated from Detectrion2 (Mask-RCNN)\\n- OUTPUT: db file that compatible to COLMAP\\n\\ncheck values of db add_matches()\'s params\\n\\n![image](https://user-images.githubusercontent.com/46152199/228710921-6f84ac5d-19ab-4652-a4b3-84b6557cadac.png)\\n\x3c!-- <img width=\\"400\\" alt=\\"image\\" src=\\"https://user-images.githubusercontent.com/46152199/228710921-6f84ac5d-19ab-4652-a4b3-84b6557cadac.png\\"> --\x3e\\narray to blob. Original array contains pair information of matching, idx1 and idx2 of each image\'s keypoint id.\\n![image](https://user-images.githubusercontent.com/46152199/228712036-4a099000-f426-46b4-9bbf-1fc439932b77.png)\\n\x3c!-- <img width=\\"400\\" alt=\\"image\\" src=\\"https://user-images.githubusercontent.com/46152199/228712036-4a099000-f426-46b4-9bbf-1fc439932b77.png\\"> --\x3e\\n\\n\\nThen, use exhaustive_matcher and mapper for sparse reconstruction.\\nThese all works well in my case.\\n\\n\\n## How to save and load feature in COLMAP.\\nPlease refer to \'DISK\' repo.\\n\\nStep 1. Feature Extraction.\\n- python detect.py h5_artifacts_destination images_directory\\n- eg. python **detect.py** --height 1024 --width 1024 --n 2048 scene/h5 scene/images\\n\\nStep 2. Match Keypoints\\n- python match.py h5_artifacts_destination\\n- eg. python **match.py** --rt 0.95 --save-threshold 100 scene/h5\\n\\nStep 3. Convert to COLMAP *.db file\\n- features are inserted to db **without descriptor**.\\n- eg. python colmap/**h5_to_db.py** --database-path scene/database.db scene/h5 scene/images\\n\\nStep 4. execute colmap cli commands\\n- **exhaustive_matcher** and **mapper** (sparse reconstruction)\\n\\n![image](https://user-images.githubusercontent.com/46152199/227842755-98eef01c-58f4-4dfe-8b62-b45adf6b03b8.png)\\n\\nThen, let\'s check how to convert h5 to db file and insert my custom features like that.\\n\\n### What kinds of information should be added to *.db file?\\nh5_to_db.py do this.\\n![image](https://user-images.githubusercontent.com/46152199/227880414-0060bb63-d1b1-4090-840c-d09431cfc80c.png){:width=\\"80%\\" height=\\"80%\\"}\\n- **db.create_tables()**\\n  - define tables that conpatible to colmap usage\\n  - by executing \'CREATE_{CAMERAS, IMAGES, KEYPOINTS, DESCRIPTORS, MATCHES, TWO_VIEW_GEOMETRIES, NAME_INDEX}_TABLE\' which are defined in colmap/colmap/database.py\\n- **add_keypoints()**\\n  - this method() returns \'fname_to_id\' (fname_to_id[filename] = image_id)\\n    - connected to db.add_keypoints(image_id, keypoints) which are defined in colmap/colmap/database.py\\n  - (optional) create_camera(): add new camera model and intrinsics\\n- **check_masking(h5_path, mask_path)**\\n  - this method() returns \'mask_keyp_id\' (mask_keyp_id[filename] = keep_inds)\\n  - read mask files and check if keypoint is valid or not, and keep this information in \'mask_keyp_id\'.\\n- **add_matches(db, h5_path, fname_to_id, mask_keyp_id)**\\n  - get colmap image ids (id_1, id_2) from fname_to_id\\n  - it calls the method db.add_matches(id_1, id_2, matches.T[masked_matches]) for image pairs, and add masked matching keypoints\\n    - matches = group[key_2][()]\\n    - i think i need to check the value. is it scaled or not, or something else...\\n    - **my custom feature pairs should fit this form!**\\n- **db.commit()**: just commit the data to the *.db file"}]}')}}]);